---
title: Microsoft 決策樹演算法技術參考 |Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- MAXIMUM_INPUT_ATTRIBUTES parameter
- SPLIT_METHOD parameter
- MINIMUM_SUPPORT parameter
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- FORCED_REGRESSOR parameter
- decision tree algorithms [Analysis Services]
- decision trees [Analysis Services]
- COMPLEXITY_PENALTY parameter
- SCORE_METHOD parameter
ms.assetid: 1e9f7969-0aa6-465a-b3ea-57b8d1c7a1fd
author: minewiskan
ms.author: owend
ms.openlocfilehash: 0cd0cd3100d0ed1213183815ae41f17cee3baa68
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 08/04/2020
ms.locfileid: "87597445"
---
# <a name="microsoft-decision-trees-algorithm-technical-reference"></a><span data-ttu-id="54e81-102">Microsoft 決策樹演算法技術參考</span><span class="sxs-lookup"><span data-stu-id="54e81-102">Microsoft Decision Trees Algorithm Technical Reference</span></span>
  <span data-ttu-id="54e81-103">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 決策樹演算法是一種混合式演算法，其中併入建立樹狀結構的不同方法，並支援多種分析工作，包括迴歸、分類以及關聯。</span><span class="sxs-lookup"><span data-stu-id="54e81-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm is a hybrid algorithm that incorporates different methods for creating a tree, and supports multiple analytic tasks, including regression, classification, and association.</span></span> <span data-ttu-id="54e81-104">Microsoft 決策樹演算法支援製作離散和連續屬性的模型。</span><span class="sxs-lookup"><span data-stu-id="54e81-104">The Microsoft Decision Trees algorithm supports modeling of both discrete and continuous attributes.</span></span>  
  
 <span data-ttu-id="54e81-105">本主題說明演算法的實作、描述如何針對不同的工作自訂演算法的行為，以及提供查詢決策樹模型其他資訊的連結。</span><span class="sxs-lookup"><span data-stu-id="54e81-105">This topic explains the implementation of the algorithm, describes how to customize the behavior of the algorithm for different tasks, and provides links to additional information about querying decision tree models.</span></span>  
  
## <a name="implementation-of-the-decision-trees-algorithm"></a><span data-ttu-id="54e81-106">實作決策樹演算法</span><span class="sxs-lookup"><span data-stu-id="54e81-106">Implementation of the Decision Trees Algorithm</span></span>  
 <span data-ttu-id="54e81-107">Microsoft 決策樹演算法將貝氏方法套用至學習因果互動模型，取得模型的近似事後分佈。</span><span class="sxs-lookup"><span data-stu-id="54e81-107">The Microsoft Decision Trees algorithm applies the Bayesian approach to learning causal interaction models by obtaining approximate posterior distributions for the models.</span></span> <span data-ttu-id="54e81-108">如需這種方法的詳細說明，請參閱 Microsoft Research 網站上由 [結構和參數學習](https://go.microsoft.com/fwlink/?LinkId=237640&clcid=0x409)提供的文件。</span><span class="sxs-lookup"><span data-stu-id="54e81-108">For a detailed explanation of this approach, see the paper on the Microsoft Research site, by [Structure and Parameter Learning](https://go.microsoft.com/fwlink/?LinkId=237640&clcid=0x409).</span></span>  
  
 <span data-ttu-id="54e81-109">評估學習所需之 *「優先」* (Priors) 資訊值的方法，是根據 *「可能性相等」*(Likelihood Equivalence) 的假設。</span><span class="sxs-lookup"><span data-stu-id="54e81-109">The methodology for assessing the information value of the *priors* needed for learning is based on the assumption of *likelihood equivalence*.</span></span> <span data-ttu-id="54e81-110">此假設認為，資料無法協助您區分代表條件式獨立之相同判斷提示的網路結構。</span><span class="sxs-lookup"><span data-stu-id="54e81-110">This assumption says that data should not help to discriminate network structures that otherwise represent the same assertions of conditional independence.</span></span> <span data-ttu-id="54e81-111">每個案例都假設擁有一個單一的貝氏優先網路以及對於該網路之信心的單一量值。</span><span class="sxs-lookup"><span data-stu-id="54e81-111">Each case is assumed to have a single Bayesian prior network and a single measure of confidence for that network.</span></span>  
  
 <span data-ttu-id="54e81-112">使用這些優先網路，演算法就可以根據目前的定型資料，計算網路結構的相對 *「事後機率」* (Posterior Probabilities)，並識別事後機率最高的網路結構。</span><span class="sxs-lookup"><span data-stu-id="54e81-112">Using these prior networks, the algorithm then computes the relative *posterior probabilities* of network structures given the current training data, and identifies the network structures that have the highest posterior probabilities.</span></span>  
  
 <span data-ttu-id="54e81-113">Microsoft 決策樹演算法使用不同的方法計算最佳的樹狀結構。</span><span class="sxs-lookup"><span data-stu-id="54e81-113">The Microsoft Decision Trees algorithm uses different methods to compute the best tree.</span></span> <span data-ttu-id="54e81-114">所使用的方法取決於工作，這可以是線性迴歸、分類或關聯分析。</span><span class="sxs-lookup"><span data-stu-id="54e81-114">The method used depends on the task, which can be linear regression, classification, or association analysis.</span></span> <span data-ttu-id="54e81-115">單一模型可以包含不同可預測屬性的多個樹狀結構。</span><span class="sxs-lookup"><span data-stu-id="54e81-115">A single model can contain multiple trees for different predictable attributes.</span></span> <span data-ttu-id="54e81-116">此外，根據資料中的屬性和值數目，每個樹狀結構都可以包含多個分支。</span><span class="sxs-lookup"><span data-stu-id="54e81-116">Moreover, each tree can contain multiple branches, depending on how many attributes and values there are in the data.</span></span> <span data-ttu-id="54e81-117">在特定模型中建立的樹狀結構形狀和深度取決於所使用的計分方法與其他參數。</span><span class="sxs-lookup"><span data-stu-id="54e81-117">The shape and depth of the tree built in a particular model depends on the scoring method and other parameters that were used.</span></span> <span data-ttu-id="54e81-118">參數中的變更也可能影響節點分岔的位置。</span><span class="sxs-lookup"><span data-stu-id="54e81-118">Changes in the parameters can also affect where the nodes split.</span></span>  
  
### <a name="building-the-tree"></a><span data-ttu-id="54e81-119">建立樹狀結構</span><span class="sxs-lookup"><span data-stu-id="54e81-119">Building the Tree</span></span>  
 <span data-ttu-id="54e81-120">當 Microsoft 決策樹演算法建立一組可能的輸入值時，該演算法會執行 *feature selection* 來識別提供多數資訊的屬性和值，並移除值非常稀少的考量。</span><span class="sxs-lookup"><span data-stu-id="54e81-120">When the Microsoft Decision Trees algorithm creates the set of possible input values, it performs *feature selection* to identify the attributes and values that provide the most information, and removes from consideration the values that are very rare.</span></span> <span data-ttu-id="54e81-121">此演算法也會將這些值分組到 *bin*中，以建立可以當做一個單位處理之值的群組，讓效能最佳化。</span><span class="sxs-lookup"><span data-stu-id="54e81-121">The algorithm also groups values into *bins*, to create groupings of values that can be processed as a unit to optimize performance.</span></span>  
  
 <span data-ttu-id="54e81-122">樹狀結構可以透過決定輸入和目標結果間的關聯來建立。</span><span class="sxs-lookup"><span data-stu-id="54e81-122">A tree is built by determining the correlations between an input and the targeted outcome.</span></span> <span data-ttu-id="54e81-123">讓所有屬性產生關聯之後，此演算法會找出最清楚分隔結果的單一屬性。</span><span class="sxs-lookup"><span data-stu-id="54e81-123">After all the attributes have been correlated, the algorithm identifies the single attribute that most cleanly separates the outcomes.</span></span> <span data-ttu-id="54e81-124">最佳分隔的這個點會使用計算資訊改善的方程式測量。</span><span class="sxs-lookup"><span data-stu-id="54e81-124">This point of the best separation is measured by using an equation that calculates information gain.</span></span> <span data-ttu-id="54e81-125">擁有資訊改善最佳分數的屬性用於將案例分割為子集，然後透過相同的程序進行遞迴性分析，直到無法再分割樹狀結構為止。</span><span class="sxs-lookup"><span data-stu-id="54e81-125">The attribute that has the best score for information gain is used to divide the cases into subsets, which are then recursively analyzed by the same process, until the tree cannot be split any more.</span></span>  
  
 <span data-ttu-id="54e81-126">用於評估資訊改善的完整方程式取決於建立演算法時所設定的參數、可預測資料行的資料類型，以及輸入的資料類型。</span><span class="sxs-lookup"><span data-stu-id="54e81-126">The exact equation used to evaluate information gain depends on the parameters set when you created the algorithm, the data type of the predictable column, and the data type of the input.</span></span>  
  
### <a name="discrete-and-continuous-inputs"></a><span data-ttu-id="54e81-127">離散和連續輸入</span><span class="sxs-lookup"><span data-stu-id="54e81-127">Discrete and Continuous Inputs</span></span>  
 <span data-ttu-id="54e81-128">當可預測的屬性和輸入都是離散的時，每個輸入的計算結果就是建立矩陣，並在矩陣中建立每個資料格之分數的結果。</span><span class="sxs-lookup"><span data-stu-id="54e81-128">When the predictable attribute is discrete and the inputs are discrete, counting the outcomes per input is a matter of creating a matrix and generating scores for each cell in the matrix.</span></span>  
  
 <span data-ttu-id="54e81-129">不過，當可預測的屬性是離散的，而輸入是連續的時，就會將連續資料行的輸入自動離散化。</span><span class="sxs-lookup"><span data-stu-id="54e81-129">However, when the predictable attribute is discrete and the inputs are continuous, the input of the continuous columns are automatically discretized.</span></span> <span data-ttu-id="54e81-130">您可以接受預設值，並讓 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 尋找最佳的 bin 數目，或者您可以設定 <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationMethod%2A> 和 <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationBucketCount%2A> 屬性來控制將連續輸入分隔的方式。</span><span class="sxs-lookup"><span data-stu-id="54e81-130">You can accept the default and have [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] find the optimum number of bins, or you can control the manner in which continuous inputs are discretized by setting the <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationMethod%2A> and <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationBucketCount%2A> properties.</span></span> <span data-ttu-id="54e81-131">如需詳細資訊，請參閱 [變更採礦模型中的資料行分隔](change-the-discretization-of-a-column-in-a-mining-model.md)。</span><span class="sxs-lookup"><span data-stu-id="54e81-131">For more information, see [Change the Discretization of a Column in a Mining Model](change-the-discretization-of-a-column-in-a-mining-model.md).</span></span>  
  
 <span data-ttu-id="54e81-132">針對連續屬性，此演算法使用線性迴歸來決定決策樹分岔之處。</span><span class="sxs-lookup"><span data-stu-id="54e81-132">For continuous attributes, the algorithm uses linear regression to determine where a decision tree splits.</span></span>  
  
 <span data-ttu-id="54e81-133">當可預測的屬性是連續數值資料類型時，也會將特徵選取套用到輸出中，以降低可能的結果數目並讓模型的建立更為快速。</span><span class="sxs-lookup"><span data-stu-id="54e81-133">When the predictable attribute is a continuous numeric data type, feature selection is applied to the outputs as well, to reduce the possible number of outcomes and build the model faster.</span></span> <span data-ttu-id="54e81-134">您可以變更特徵選取的臨界值，並藉此設定 MAXIMUM_OUTPUT_ATTRIBUTES 參數來增加或減少可能值的數目。</span><span class="sxs-lookup"><span data-stu-id="54e81-134">You can change the threshold for feature selection and thereby increase or decrease the number of possible values by setting the MAXIMUM_OUTPUT_ATTRIBUTES parameter.</span></span>  
  
 <span data-ttu-id="54e81-135">如需有關 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 決策樹演算法如何與分隔可預測的資料行搭配使用的詳細說明，請參閱＜ [學習貝氏網路：知識與統計資料的組合](https://go.microsoft.com/fwlink/?LinkId=45963)。</span><span class="sxs-lookup"><span data-stu-id="54e81-135">For a more detained explanation about how the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm works with discrete predictable columns, see [Learning Bayesian Networks: The Combination of Knowledge and Statistical Data](https://go.microsoft.com/fwlink/?LinkId=45963).</span></span> <span data-ttu-id="54e81-136">如需 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 決策樹演算法如何與連續可預測資料行一起運作的詳細資訊，請參閱 [Autoregressive Tree Models for Time-Series Analysis](https://go.microsoft.com/fwlink/?LinkId=45966)(時間序列分析的自動迴歸樹狀模型) 的附錄。</span><span class="sxs-lookup"><span data-stu-id="54e81-136">For more information about how the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm works with a continuous predictable column, see the appendix of [Autoregressive Tree Models for Time-Series Analysis](https://go.microsoft.com/fwlink/?LinkId=45966).</span></span>  
  
### <a name="scoring-methods-and-feature-selection"></a><span data-ttu-id="54e81-137">計分方法與特徵選取</span><span class="sxs-lookup"><span data-stu-id="54e81-137">Scoring Methods and Feature Selection</span></span>  
 <span data-ttu-id="54e81-138">Microsoft 決策樹演算法提供三個計算資訊改善分數的公式：「Shannon 熵」、「使用 K2 優先的貝氏網路」以及「使用優先統一狄氏分配的貝氏網路」。</span><span class="sxs-lookup"><span data-stu-id="54e81-138">The Microsoft Decision Trees algorithm offers three formulas for scoring information gain: Shannon's entropy, Bayesian network with K2 prior, and Bayesian network with a uniform Dirichlet distribution of priors.</span></span> <span data-ttu-id="54e81-139">三種方法全都堅實的建立在資料採礦欄位中。</span><span class="sxs-lookup"><span data-stu-id="54e81-139">All three methods are well established in the data mining field.</span></span> <span data-ttu-id="54e81-140">建議您試驗不同的參數與計分方法來判斷哪個公式會提供最佳的結果。</span><span class="sxs-lookup"><span data-stu-id="54e81-140">We recommend that you experiment with different parameters and scoring methods to determine which provides the best results.</span></span> <span data-ttu-id="54e81-141">如需有關這些計分方法的詳細資訊，請參閱＜ [Feature Selection](../../sql-server/install/feature-selection.md)＞。</span><span class="sxs-lookup"><span data-stu-id="54e81-141">For more information about these scoring methods, see [Feature Selection](../../sql-server/install/feature-selection.md).</span></span>  
  
 <span data-ttu-id="54e81-142">所有 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 資料採礦演算法都會自動使用特徵選取來改善分析並減少處理的負載。</span><span class="sxs-lookup"><span data-stu-id="54e81-142">All [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms automatically use feature selection to improve analysis and reduce processing load.</span></span> <span data-ttu-id="54e81-143">特徵選取所使用的方法取決於建立模型所使用的演算法。</span><span class="sxs-lookup"><span data-stu-id="54e81-143">The method used for feature selection depends on the algorithm that is used to build the model.</span></span> <span data-ttu-id="54e81-144">針對決策樹模型控制特徵選取的演算法參數為 MAXIMUM_INPUT_ATTRIBUTES 和 MAXIMUM_OUTPUT。</span><span class="sxs-lookup"><span data-stu-id="54e81-144">The algorithm parameters that control feature selection for a decision trees model are MAXIMUM_INPUT_ATTRIBUTES and MAXIMUM_OUTPUT.</span></span>  
  
|<span data-ttu-id="54e81-145">演算法</span><span class="sxs-lookup"><span data-stu-id="54e81-145">Algorithm</span></span>|<span data-ttu-id="54e81-146">分析的方法</span><span class="sxs-lookup"><span data-stu-id="54e81-146">Method of analysis</span></span>|<span data-ttu-id="54e81-147">註解</span><span class="sxs-lookup"><span data-stu-id="54e81-147">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="54e81-148">決策樹</span><span class="sxs-lookup"><span data-stu-id="54e81-148">Decision Trees</span></span>|<span data-ttu-id="54e81-149">有趣性分數</span><span class="sxs-lookup"><span data-stu-id="54e81-149">Interestingness score</span></span><br /><br /> <span data-ttu-id="54e81-150">Shannon 熵</span><span class="sxs-lookup"><span data-stu-id="54e81-150">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="54e81-151">使用 K2 優先的貝氏</span><span class="sxs-lookup"><span data-stu-id="54e81-151">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="54e81-152">使用優先統一狄氏分配的貝氏 (預設值)</span><span class="sxs-lookup"><span data-stu-id="54e81-152">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="54e81-153">如果任何資料行包含非二進位連續數值，則所有資料行都會使用有趣性分數以確保一致性。</span><span class="sxs-lookup"><span data-stu-id="54e81-153">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="54e81-154">否則，會使用預設值或指定的方法。</span><span class="sxs-lookup"><span data-stu-id="54e81-154">Otherwise, the default or specified method is used.</span></span>|  
|<span data-ttu-id="54e81-155">線性迴歸</span><span class="sxs-lookup"><span data-stu-id="54e81-155">Linear Regression</span></span>|<span data-ttu-id="54e81-156">有趣性分數</span><span class="sxs-lookup"><span data-stu-id="54e81-156">Interestingness score</span></span>|<span data-ttu-id="54e81-157">線性迴歸只會使用有趣性，因為它只支援連續的資料行。</span><span class="sxs-lookup"><span data-stu-id="54e81-157">Linear Regression only uses interestingness, because it only supports continuous columns.</span></span>|  
  
### <a name="scalability-and-performance"></a><span data-ttu-id="54e81-158">延展性和效能</span><span class="sxs-lookup"><span data-stu-id="54e81-158">Scalability and Performance</span></span>  
 <span data-ttu-id="54e81-159">分類是一個重要的資料採擷策略。</span><span class="sxs-lookup"><span data-stu-id="54e81-159">Classification is an important data mining strategy.</span></span> <span data-ttu-id="54e81-160">一般而言，為案例分類所需之資訊的數量會以輸入記錄數目的直接比例成本。</span><span class="sxs-lookup"><span data-stu-id="54e81-160">Generally, the amount of information that is needed to classify the cases grows in direct proportion to the number of input records.</span></span> <span data-ttu-id="54e81-161">這會限制可以分類之資料的大小。</span><span class="sxs-lookup"><span data-stu-id="54e81-161">This limits the size of the data that can be classified.</span></span> <span data-ttu-id="54e81-162">Microsoft 決策樹演算法使用下列方法解決這些問題、改善效能，並排除記憶體限制。</span><span class="sxs-lookup"><span data-stu-id="54e81-162">The Microsoft Decision Trees algorithm using uses the following methods to resolve these problems, improve performance, and eliminate memory restrictions:</span></span>  
  
-   <span data-ttu-id="54e81-163">特徵選取，用於最佳化屬性選取。</span><span class="sxs-lookup"><span data-stu-id="54e81-163">Feature selection to optimize the selection of attributes.</span></span>  
  
-   <span data-ttu-id="54e81-164">貝氏計分，用於控制樹狀結構的成長。</span><span class="sxs-lookup"><span data-stu-id="54e81-164">Bayesian scoring to control tree growth.</span></span>  
  
-   <span data-ttu-id="54e81-165">bin 最佳化，用於連續屬性。</span><span class="sxs-lookup"><span data-stu-id="54e81-165">Optimization of binning for continuous attributes.</span></span>  
  
-   <span data-ttu-id="54e81-166">輸入值的動態群組，用於判斷最重要的值。</span><span class="sxs-lookup"><span data-stu-id="54e81-166">Dynamic grouping of input values to determine the most important values.</span></span>  
  
 <span data-ttu-id="54e81-167">Microsoft 決策樹演算法快速、可擴充，而且針對輕鬆平行處理而設計，也就是說，所有處理器會一同運作以建立單一而且一致的模型。</span><span class="sxs-lookup"><span data-stu-id="54e81-167">The Microsoft Decision Trees algorithm is fast and scalable, and has been designed to be easily parallelized, meaning that all processors work together to build a single, consistent model.</span></span> <span data-ttu-id="54e81-168">這些特徵的結合可讓決策樹分類成為資料採礦的理想工具。</span><span class="sxs-lookup"><span data-stu-id="54e81-168">The combination of these characteristics makes the decision-tree classifier an ideal tool for data mining.</span></span>  
  
 <span data-ttu-id="54e81-169">如果效能限制嚴苛，您可以在定期決策樹模型期間，使用下列方法改善處理時間。</span><span class="sxs-lookup"><span data-stu-id="54e81-169">If performance constraints are severe, you might be able to improve processing time during the training of a decision tree model by using the following methods.</span></span> <span data-ttu-id="54e81-170">不過，如果您這麼做，請注意，刪除屬性來改善處理效能將會變更模型的結果，而且可能比較不能代表總母體。</span><span class="sxs-lookup"><span data-stu-id="54e81-170">However, if you do so, be aware that eliminating attributes to improve processing performance will change the results of the model, and possibly make it less representative of the total population.</span></span>  
  
-   <span data-ttu-id="54e81-171">增加 COMPLEXITY_PENALTY 參數的值來限制樹狀結構的成長。</span><span class="sxs-lookup"><span data-stu-id="54e81-171">Increase the value of the COMPLEXITY_PENALTY parameter to limit tree growth.</span></span>  
  
-   <span data-ttu-id="54e81-172">限制關聯模型中的項目數目以限制所建立的樹狀結構數目。</span><span class="sxs-lookup"><span data-stu-id="54e81-172">Limit the number of items in association models to limit the number of trees that are built.</span></span>  
  
-   <span data-ttu-id="54e81-173">增加 MINIMUM_SUPPORT 參數的值以防止過度膨脹。</span><span class="sxs-lookup"><span data-stu-id="54e81-173">Increase the value of the MINIMUM_SUPPORT parameter to avoid overfitting.</span></span>  
  
-   <span data-ttu-id="54e81-174">將任何屬性之離散值的數目限制為 10 以下。</span><span class="sxs-lookup"><span data-stu-id="54e81-174">Restrict the number of discrete values for any attribute to 10 or less.</span></span> <span data-ttu-id="54e81-175">您可以嘗試在不同的模型中，嘗試以不同的方式為這些值分組。</span><span class="sxs-lookup"><span data-stu-id="54e81-175">You might try grouping values in different ways in different models.</span></span>  
  
    > [!NOTE]  
    >  <span data-ttu-id="54e81-176">您可以使用  [!INCLUDE[ssISCurrent](../../includes/ssiscurrent-md.md)] 中所提供的資料瀏覽工具，將資料中的值分佈視覺化，並在開始資料採礦前，將您的值正確分組。</span><span class="sxs-lookup"><span data-stu-id="54e81-176">You can use the data exploration tools available in  [!INCLUDE[ssISCurrent](../../includes/ssiscurrent-md.md)] to visualize the distribution of values in your data and group your values appropriately before beginning data mining.</span></span> <span data-ttu-id="54e81-177">如需詳細資訊，請參閱 [資料分析工作和檢視器](../../integration-services/control-flow/data-profiling-task-and-viewer.md)。</span><span class="sxs-lookup"><span data-stu-id="54e81-177">For more information, see [Data Profiling Task and Viewer](../../integration-services/control-flow/data-profiling-task-and-viewer.md).</span></span> <span data-ttu-id="54e81-178">您也可以使用 [適用於 Excel 2007 的資料採礦增益集](https://www.microsoft.com/download/details.aspx?id=8569)瀏覽、分組，以及重新標示 Microsoft Excel 中的資料。</span><span class="sxs-lookup"><span data-stu-id="54e81-178">You can also use the [Data Mining Add-ins for Excel 2007](https://www.microsoft.com/download/details.aspx?id=8569), to explore, group and relabel data in Microsoft Excel.</span></span>  
  
## <a name="customizing-the-decision-trees-algorithm"></a><span data-ttu-id="54e81-179">自訂決策樹演算法</span><span class="sxs-lookup"><span data-stu-id="54e81-179">Customizing the Decision Trees Algorithm</span></span>  
 <span data-ttu-id="54e81-180">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 決策樹演算法支援會影響所產生之採礦模型效能和精確度的參數。</span><span class="sxs-lookup"><span data-stu-id="54e81-180">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports parameters that affect the performance and accuracy of the resulting mining model.</span></span> <span data-ttu-id="54e81-181">您也可以設定採礦模型資料行或採礦結構資料行上的模型旗標來控制處理資料的方式。</span><span class="sxs-lookup"><span data-stu-id="54e81-181">You can also set modeling flags on the mining model columns or mining structure columns to control the way that data is processed.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="54e81-182">Microsoft 決策樹演算法可用於所有 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]版本中，但是 Microsoft 決策樹演算法自訂行為的某些進階參數只能在特定 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]版本中使用。</span><span class="sxs-lookup"><span data-stu-id="54e81-182">The Microsoft Decision Trees algorithm is available in all editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]; however, some advanced parameters for customizing the behavior of the Microsoft Decision Trees algorithm are available for use only in specific editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)].</span></span> <span data-ttu-id="54e81-183">如需版本支援的功能清單 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] ，請參閱[SQL Server 2012 (版本支援的功能](https://go.microsoft.com/fwlink/?linkid=232473) https://go.microsoft.com/fwlink/?linkid=232473) 。</span><span class="sxs-lookup"><span data-stu-id="54e81-183">For a list of features that are supported by the editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)], see [Features Supported by the Editions of SQL Server 2012](https://go.microsoft.com/fwlink/?linkid=232473) (https://go.microsoft.com/fwlink/?linkid=232473).</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="54e81-184">設定演算法參數</span><span class="sxs-lookup"><span data-stu-id="54e81-184">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="54e81-185">下表描述可以搭配 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 決策樹演算法使用的參數。</span><span class="sxs-lookup"><span data-stu-id="54e81-185">The following table describes the parameters that you can use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm.</span></span>  
  
 <span data-ttu-id="54e81-186">*COMPLEXITY_PENALTY*</span><span class="sxs-lookup"><span data-stu-id="54e81-186">*COMPLEXITY_PENALTY*</span></span>  
 <span data-ttu-id="54e81-187">控制決策樹的成長。</span><span class="sxs-lookup"><span data-stu-id="54e81-187">Controls the growth of the decision tree.</span></span> <span data-ttu-id="54e81-188">低值會增加分岔數目，而高值會減少分岔數目。</span><span class="sxs-lookup"><span data-stu-id="54e81-188">A low value increases the number of splits, and a high value decreases the number of splits.</span></span> <span data-ttu-id="54e81-189">預設值是依據特定模型的屬性數目，如下列清單所述：</span><span class="sxs-lookup"><span data-stu-id="54e81-189">The default value is based on the number of attributes for a particular model, as described in the following list:</span></span>  
  
-   <span data-ttu-id="54e81-190">1 到 9 個屬性，預設值為 0.5。</span><span class="sxs-lookup"><span data-stu-id="54e81-190">For 1 through 9 attributes, the default is 0.5.</span></span>  
  
-   <span data-ttu-id="54e81-191">10 到 99 個屬性，預設值為 0.9。</span><span class="sxs-lookup"><span data-stu-id="54e81-191">For 10 through 99 attributes, the default is 0.9.</span></span>  
  
-   <span data-ttu-id="54e81-192">100 個以上的屬性，預設值為 0.99。</span><span class="sxs-lookup"><span data-stu-id="54e81-192">For 100 or more attributes, the default is 0.99.</span></span>  
  
 <span data-ttu-id="54e81-193">*FORCE_REGRESSOR*</span><span class="sxs-lookup"><span data-stu-id="54e81-193">*FORCE_REGRESSOR*</span></span>  
 <span data-ttu-id="54e81-194">強制演算法使用指定的資料行做為迴歸輸入變數，不考慮演算法計算出來之資料行的重要性。</span><span class="sxs-lookup"><span data-stu-id="54e81-194">Forces the algorithm to use the specified columns as regressors, regardless of the importance of the columns as calculated by the algorithm.</span></span> <span data-ttu-id="54e81-195">此參數只用於預測連續屬性的決策樹。</span><span class="sxs-lookup"><span data-stu-id="54e81-195">This parameter is only used for decision trees that are predicting a continuous attribute.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="54e81-196">您可以設定這個屬性來強制演算法嘗試使用屬性當做迴歸輸入變數。</span><span class="sxs-lookup"><span data-stu-id="54e81-196">By setting this parameter, you force the algorithm to try to use the attribute as a regressor.</span></span> <span data-ttu-id="54e81-197">不過，屬性實際上在最終模型中是否當做迴歸輸入變數使用，則視分析的結果而定。</span><span class="sxs-lookup"><span data-stu-id="54e81-197">However, whether the attribute is actually used as a regressor in the final model depends on the results of analysis.</span></span> <span data-ttu-id="54e81-198">您可以查詢模型內容，找出當做迴歸輸入變數使用的資料行。</span><span class="sxs-lookup"><span data-stu-id="54e81-198">You can find out which columns were used as regressors by querying the model content.</span></span>  
  
 <span data-ttu-id="54e81-199">[只有在某些 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] 版本中才可使用]</span><span class="sxs-lookup"><span data-stu-id="54e81-199">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] ]</span></span>  
  
 <span data-ttu-id="54e81-200">*MAXIMUM_INPUT_ATTRIBUTES*</span><span class="sxs-lookup"><span data-stu-id="54e81-200">*MAXIMUM_INPUT_ATTRIBUTES*</span></span>  
 <span data-ttu-id="54e81-201">定義演算法在叫用特徵選取之前，可以處理的輸入屬性數目。</span><span class="sxs-lookup"><span data-stu-id="54e81-201">Defines the number of input attributes that the algorithm can handle before it invokes feature selection.</span></span>  
  
 <span data-ttu-id="54e81-202">預設值為 255。</span><span class="sxs-lookup"><span data-stu-id="54e81-202">The default is 255.</span></span>  
  
 <span data-ttu-id="54e81-203">將此值設定為 0 可關閉特徵選取。</span><span class="sxs-lookup"><span data-stu-id="54e81-203">Set this value to 0 to turn off feature selection.</span></span>  
  
 <span data-ttu-id="54e81-204">[只有在某些 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]版本中才可使用]</span><span class="sxs-lookup"><span data-stu-id="54e81-204">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span></span>  
  
 <span data-ttu-id="54e81-205">*MAXIMUM_OUTPUT_ATTRIBUTES*</span><span class="sxs-lookup"><span data-stu-id="54e81-205">*MAXIMUM_OUTPUT_ATTRIBUTES*</span></span>  
 <span data-ttu-id="54e81-206">定義演算法在叫用特徵選取之前，可以處理的輸出屬性數目。</span><span class="sxs-lookup"><span data-stu-id="54e81-206">Defines the number of output attributes that the algorithm can handle before it invokes feature selection.</span></span>  
  
 <span data-ttu-id="54e81-207">預設值為 255。</span><span class="sxs-lookup"><span data-stu-id="54e81-207">The default is 255.</span></span>  
  
 <span data-ttu-id="54e81-208">將此值設定為 0 可關閉特徵選取。</span><span class="sxs-lookup"><span data-stu-id="54e81-208">Set this value to 0 to turn off feature selection.</span></span>  
  
 <span data-ttu-id="54e81-209">[只有在某些 [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]版本中才可使用]</span><span class="sxs-lookup"><span data-stu-id="54e81-209">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span></span>  
  
 <span data-ttu-id="54e81-210">*MINIMUM_SUPPORT*</span><span class="sxs-lookup"><span data-stu-id="54e81-210">*MINIMUM_SUPPORT*</span></span>  
 <span data-ttu-id="54e81-211">決定要在決策樹中產生分岔所需的最小分葉案例數目。</span><span class="sxs-lookup"><span data-stu-id="54e81-211">Determines the minimum number of leaf cases that is required to generate a split in the decision tree.</span></span>  
  
 <span data-ttu-id="54e81-212">預設值為 10。</span><span class="sxs-lookup"><span data-stu-id="54e81-212">The default is 10.</span></span>  
  
 <span data-ttu-id="54e81-213">如果資料集非常大，您可能需要增加這個值以防止過度膨脹。</span><span class="sxs-lookup"><span data-stu-id="54e81-213">You may need to increase this value if the dataset is very large, to avoid overtraining.</span></span>  
  
 <span data-ttu-id="54e81-214">*SCORE_METHOD*</span><span class="sxs-lookup"><span data-stu-id="54e81-214">*SCORE_METHOD*</span></span>  
 <span data-ttu-id="54e81-215">決定用來計算分岔準則的方法。</span><span class="sxs-lookup"><span data-stu-id="54e81-215">Determines the method that is used to calculate the split score.</span></span> <span data-ttu-id="54e81-216">有下列選項可供使用：</span><span class="sxs-lookup"><span data-stu-id="54e81-216">The following options are available:</span></span>  
  
|<span data-ttu-id="54e81-217">ID</span><span class="sxs-lookup"><span data-stu-id="54e81-217">ID</span></span>|<span data-ttu-id="54e81-218">名稱</span><span class="sxs-lookup"><span data-stu-id="54e81-218">Name</span></span>|  
|--------|----------|  
|<span data-ttu-id="54e81-219">1</span><span class="sxs-lookup"><span data-stu-id="54e81-219">1</span></span>|<span data-ttu-id="54e81-220">熵</span><span class="sxs-lookup"><span data-stu-id="54e81-220">Entropy</span></span>|  
|<span data-ttu-id="54e81-221">3</span><span class="sxs-lookup"><span data-stu-id="54e81-221">3</span></span>|<span data-ttu-id="54e81-222">使用 K2 優先的貝氏</span><span class="sxs-lookup"><span data-stu-id="54e81-222">Bayesian with K2 Prior</span></span>|  
|<span data-ttu-id="54e81-223">4</span><span class="sxs-lookup"><span data-stu-id="54e81-223">4</span></span>|<span data-ttu-id="54e81-224">使用統一優先的貝氏狄氏等價 (Bayesian Dirichlet Equivalent，BDE)</span><span class="sxs-lookup"><span data-stu-id="54e81-224">Bayesian Dirichlet Equivalent (BDE) with uniform prior</span></span><br /><br /> <span data-ttu-id="54e81-225">(預設值)</span><span class="sxs-lookup"><span data-stu-id="54e81-225">(default)</span></span>|  
  
 <span data-ttu-id="54e81-226">預設值是 4 或 BDE。</span><span class="sxs-lookup"><span data-stu-id="54e81-226">The default is 4, or BDE.</span></span>  
  
 <span data-ttu-id="54e81-227">如需這些計分方法的說明，請參閱＜ [Feature Selection](../../sql-server/install/feature-selection.md)＞。</span><span class="sxs-lookup"><span data-stu-id="54e81-227">For an explanation of these scoring methods, see [Feature Selection](../../sql-server/install/feature-selection.md).</span></span>  
  
 <span data-ttu-id="54e81-228">*SPLIT_METHOD*</span><span class="sxs-lookup"><span data-stu-id="54e81-228">*SPLIT_METHOD*</span></span>  
 <span data-ttu-id="54e81-229">決定用來分岔節點的方法。</span><span class="sxs-lookup"><span data-stu-id="54e81-229">Determines the method that is used to split the node.</span></span> <span data-ttu-id="54e81-230">有下列選項可供使用：</span><span class="sxs-lookup"><span data-stu-id="54e81-230">The following options are available:</span></span>  
  
|<span data-ttu-id="54e81-231">ID</span><span class="sxs-lookup"><span data-stu-id="54e81-231">ID</span></span>|<span data-ttu-id="54e81-232">名稱</span><span class="sxs-lookup"><span data-stu-id="54e81-232">Name</span></span>|  
|--------|----------|  
|<span data-ttu-id="54e81-233">1</span><span class="sxs-lookup"><span data-stu-id="54e81-233">1</span></span>|<span data-ttu-id="54e81-234">**Binary:** 表示不管屬性的實際數目為何，樹狀結構都會分岔為兩個分支。</span><span class="sxs-lookup"><span data-stu-id="54e81-234">**Binary:** Indicates that regardless of the actual number of values for the attribute, the tree should be split into two branches.</span></span>|  
|<span data-ttu-id="54e81-235">2</span><span class="sxs-lookup"><span data-stu-id="54e81-235">2</span></span>|<span data-ttu-id="54e81-236">**Complete:** 表示樹狀結構可以建立與屬性值一樣多的分岔。</span><span class="sxs-lookup"><span data-stu-id="54e81-236">**Complete:** Indicates that the tree can create as many splits as there are attribute values.</span></span>|  
|<span data-ttu-id="54e81-237">3</span><span class="sxs-lookup"><span data-stu-id="54e81-237">3</span></span>|<span data-ttu-id="54e81-238">**Both:** 指定 Analysis Services 可以決定應該使用二進位還是完整分岔來產生最佳的結果。</span><span class="sxs-lookup"><span data-stu-id="54e81-238">**Both:** Specifies that Analysis Services can determine whether a binary or complete split should be used to produce the best results.</span></span>|  
  
 <span data-ttu-id="54e81-239">預設值為 3。</span><span class="sxs-lookup"><span data-stu-id="54e81-239">The default is 3.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="54e81-240">模型旗標</span><span class="sxs-lookup"><span data-stu-id="54e81-240">Modeling Flags</span></span>  
 <span data-ttu-id="54e81-241">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 決策樹演算法支援下列模型旗標。</span><span class="sxs-lookup"><span data-stu-id="54e81-241">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports the following modeling flags.</span></span> <span data-ttu-id="54e81-242">當您建立採礦結構或採礦模型時，您會定義模型旗標來指定分析期間要如何處理每個資料行中的值。</span><span class="sxs-lookup"><span data-stu-id="54e81-242">When you create the mining structure or mining model, you define modeling flags to specify how values in each column are handled during analysis.</span></span> <span data-ttu-id="54e81-243">如需詳細資訊，請參閱[模型旗標 &#40;資料採礦&#41;](modeling-flags-data-mining.md)。</span><span class="sxs-lookup"><span data-stu-id="54e81-243">For more information, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md).</span></span>  
  
|<span data-ttu-id="54e81-244">模型旗標</span><span class="sxs-lookup"><span data-stu-id="54e81-244">Modeling Flag</span></span>|<span data-ttu-id="54e81-245">描述</span><span class="sxs-lookup"><span data-stu-id="54e81-245">Description</span></span>|  
|-------------------|-----------------|  
|<span data-ttu-id="54e81-246">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="54e81-246">MODEL_EXISTENCE_ONLY</span></span>|<span data-ttu-id="54e81-247">表示資料行將被視為擁有兩個可能狀態：`Missing` 和 `Existing`。</span><span class="sxs-lookup"><span data-stu-id="54e81-247">Means that the column will be treated as having two possible states: `Missing` and `Existing`.</span></span> <span data-ttu-id="54e81-248">Null 為遺漏值。</span><span class="sxs-lookup"><span data-stu-id="54e81-248">A null is a missing value.</span></span><br /><br /> <span data-ttu-id="54e81-249">適用於採礦模型資料行。</span><span class="sxs-lookup"><span data-stu-id="54e81-249">Applies to mining model columns.</span></span>|  
|<span data-ttu-id="54e81-250">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="54e81-250">NOT NULL</span></span>|<span data-ttu-id="54e81-251">表示資料行不能包含 Null 值。</span><span class="sxs-lookup"><span data-stu-id="54e81-251">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="54e81-252">如果 Analysis Services 在模型定型期間遇到 Null 值，將會產生錯誤。</span><span class="sxs-lookup"><span data-stu-id="54e81-252">An error will result if Analysis Services encounters a null during model training.</span></span><br /><br /> <span data-ttu-id="54e81-253">適用於採礦結構資料行。</span><span class="sxs-lookup"><span data-stu-id="54e81-253">Applies to mining structure columns.</span></span>|  
  
### <a name="regressors-in-decision-tree-models"></a><span data-ttu-id="54e81-254">決策樹模型中的迴歸輸入變數</span><span class="sxs-lookup"><span data-stu-id="54e81-254">Regressors in Decision Tree Models</span></span>  
 <span data-ttu-id="54e81-255">即使您不使用 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 線性迴歸演算法，包含連續數值輸入和輸出的任何決策樹模型潛在可能包含代表連續屬性迴歸的節點。</span><span class="sxs-lookup"><span data-stu-id="54e81-255">Even if you do not use the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Linear Regression algorithm, any decision tree model that has continuous numeric inputs and outputs can potentially include nodes that represent a regression on a continuous attribute.</span></span>  
  
 <span data-ttu-id="54e81-256">您不需要指定連續數值資料的資料行代表迴歸輸入變數。</span><span class="sxs-lookup"><span data-stu-id="54e81-256">You do not need to specify that a column of continuous numeric data represents a regressor.</span></span> <span data-ttu-id="54e81-257">即使未在資料行上設定 REGRESSOR 旗標， [!INCLUDE[msCoName](../../includes/msconame-md.md)] 決策樹演算法也會自動使用資料行當做潛在迴歸輸入變數，並將資料集分割成具備有意義之模式的區域。</span><span class="sxs-lookup"><span data-stu-id="54e81-257">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm will automatically use the column as a potential regressor and partition the dataset into regions with meaningful patterns even if you do not set the REGRESSOR flag on the column.</span></span>  
  
 <span data-ttu-id="54e81-258">不過，您可以使用 FORCE_REGRESSOR 參數來確保演算法會使用特定的迴歸輸入變數。</span><span class="sxs-lookup"><span data-stu-id="54e81-258">However, you can use the FORCE_REGRESSOR parameter to guarantee that the algorithm will use a particular regressor.</span></span> <span data-ttu-id="54e81-259">這個參數僅能搭配 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 決策樹演算法及 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 線性迴歸演算法使用。</span><span class="sxs-lookup"><span data-stu-id="54e81-259">This parameter can be used only with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees and [!INCLUDE[msCoName](../../includes/msconame-md.md)] Linear Regression algorithms.</span></span> <span data-ttu-id="54e81-260">當您設定模型旗標時，演算法會嘗試尋找表單 a \* C1 + b \* C2 + ... 的回歸方公式。以符合樹狀結構節點中的模式。</span><span class="sxs-lookup"><span data-stu-id="54e81-260">When you set the modeling flag, the algorithm will try to find regression equations of the form a\*C1 + b\*C2 + ... to fit the patterns in the nodes of the tree.</span></span> <span data-ttu-id="54e81-261">之後會計算剩餘數的總和，如果差異過大，就會在樹狀結構中強制進行分割。</span><span class="sxs-lookup"><span data-stu-id="54e81-261">The sum of the residuals is calculated, and if the deviation is too great, a split is forced in the tree.</span></span>  
  
 <span data-ttu-id="54e81-262">例如，如果您要使用 **Income** 做為屬性來預測客戶購買行為，且在資料行上設定 REGRESSOR 模型旗標，則演算法首先會使用標準迴歸公式來比對 **Income** 值。</span><span class="sxs-lookup"><span data-stu-id="54e81-262">For example, if you are predicting customer purchasing behavior using **Income** as an attribute, and set the REGRESSOR modeling flag on the column, the algorithm will first try to fit the **Income** values by using a standard regression formula.</span></span> <span data-ttu-id="54e81-263">如果差異過大，就會放棄迴歸公式，且根據其他的屬性分割樹狀結構。</span><span class="sxs-lookup"><span data-stu-id="54e81-263">If the deviation is too great, the regression formula is abandoned and the tree will be split on another attribute.</span></span> <span data-ttu-id="54e81-264">在分割之後，決策樹演算法就會接著嘗試在每個分支中比對迴歸輸入變數與收入。</span><span class="sxs-lookup"><span data-stu-id="54e81-264">The decision tree algorithm will then try to fit a regressor for income in each of the branches after the split.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="54e81-265">需求</span><span class="sxs-lookup"><span data-stu-id="54e81-265">Requirements</span></span>  
 <span data-ttu-id="54e81-266">決策樹模型必須包含索引鍵資料行、輸入資料行和至少一個可預測資料行。</span><span class="sxs-lookup"><span data-stu-id="54e81-266">A decision tree model must contain a key column, input columns, and at least one predictable column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="54e81-267">輸入和可預測資料行</span><span class="sxs-lookup"><span data-stu-id="54e81-267">Input and Predictable Columns</span></span>  
 <span data-ttu-id="54e81-268">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 決策樹演算法支援下表所列的特定輸入資料行和可預測資料行。</span><span class="sxs-lookup"><span data-stu-id="54e81-268">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span> <span data-ttu-id="54e81-269">如需內容類型用於採礦模型時所代表意義的詳細資訊，請參閱[內容類型 &#40;資料採礦&#41;](content-types-data-mining.md)。</span><span class="sxs-lookup"><span data-stu-id="54e81-269">For more information about what the content types mean when used in a mining model, see [Content Types &#40;Data Mining&#41;](content-types-data-mining.md).</span></span>  
  
|<span data-ttu-id="54e81-270">資料行</span><span class="sxs-lookup"><span data-stu-id="54e81-270">Column</span></span>|<span data-ttu-id="54e81-271">內容類型</span><span class="sxs-lookup"><span data-stu-id="54e81-271">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="54e81-272">輸入屬性</span><span class="sxs-lookup"><span data-stu-id="54e81-272">Input attribute</span></span>|<span data-ttu-id="54e81-273">Continuous、Cyclical、Discrete、Discretized、Key、Ordered、Table</span><span class="sxs-lookup"><span data-stu-id="54e81-273">Continuous, Cyclical, Discrete, Discretized, Key, Ordered, Table</span></span>|  
|<span data-ttu-id="54e81-274">可預測屬性</span><span class="sxs-lookup"><span data-stu-id="54e81-274">Predictable attribute</span></span>|<span data-ttu-id="54e81-275">Continuous、Cyclical、Discrete、Discretized、Ordered、Table</span><span class="sxs-lookup"><span data-stu-id="54e81-275">Continuous, Cyclical, Discrete, Discretized, Ordered, Table</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="54e81-276">系統支援 Cyclical 和 Ordered 內容類型，但是演算法將它們視為離散值，因此不會執行特殊處理。</span><span class="sxs-lookup"><span data-stu-id="54e81-276">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="54e81-277">另請參閱</span><span class="sxs-lookup"><span data-stu-id="54e81-277">See Also</span></span>  
 <span data-ttu-id="54e81-278">[Microsoft 決策樹演算法](microsoft-decision-trees-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="54e81-278">[Microsoft Decision Trees Algorithm](microsoft-decision-trees-algorithm.md) </span></span>  
 <span data-ttu-id="54e81-279">[決策樹模型查詢範例](decision-trees-model-query-examples.md) </span><span class="sxs-lookup"><span data-stu-id="54e81-279">[Decision Trees Model Query Examples](decision-trees-model-query-examples.md) </span></span>  
 [<span data-ttu-id="54e81-280">決策樹模型的採礦模型內容 &#40;Analysis Services - 資料採礦&#41;</span><span class="sxs-lookup"><span data-stu-id="54e81-280">Mining Model Content for Decision Tree Models &#40;Analysis Services - Data Mining&#41;</span></span>](mining-model-content-for-decision-tree-models-analysis-services-data-mining.md)  
  
  
