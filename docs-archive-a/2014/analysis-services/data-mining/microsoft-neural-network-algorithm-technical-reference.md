---
title: Microsoft 類神經網路演算法技術參考 |Microsoft Docs
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- HIDDEN_NODE_RATIO parameter
- MAXIMUM_INPUT_ATTRIBUTES parameter
- HOLDOUT_PERCENTAGE parameter
- neural network algorithms [Analysis Services]
- output layer [Data Mining]
- neural networks
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- MAXIMUM_STATES parameter
- SAMPLE_SIZE parameter
- hidden layer
- hidden neurons
- input layer [Data Mining]
- activation function [Data Mining]
- Back-Propagated Delta Rule network
- neural network model [Analysis Services]
- coding [Data Mining]
- HOLDOUT_SEED parameter
ms.assetid: b8fac409-e3c0-4216-b032-364f8ea51095
author: minewiskan
ms.author: owend
ms.openlocfilehash: 3c36fd9f3446ddf36da9af7ce58259edbe84c8cf
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 08/04/2020
ms.locfileid: "87584844"
---
# <a name="microsoft-neural-network-algorithm-technical-reference"></a><span data-ttu-id="28a8b-102">Microsoft 類神經網路演算法技術參考資料</span><span class="sxs-lookup"><span data-stu-id="28a8b-102">Microsoft Neural Network Algorithm Technical Reference</span></span>
  <span data-ttu-id="28a8b-103">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 類神經網路使用「多層認知」**(Multilayer Perceptron) 網路，亦稱為「倒傳播差異規則」**(Back-Propagated Delta Rule) 網路，它包含最多 3 層神經 (Neuron) 或「認知器」\*\*(Perceptron)。</span><span class="sxs-lookup"><span data-stu-id="28a8b-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network uses a *Multilayer Perceptron* network, also called a *Back-Propagated Delta Rule network*, composed of up to three layers of neurons, or *perceptrons*.</span></span> <span data-ttu-id="28a8b-104">這 3 層分別是輸入層、選擇性隱藏層和輸出層。</span><span class="sxs-lookup"><span data-stu-id="28a8b-104">These layers are an input layer, an optional hidden layer, and an output layer.</span></span>  
  
 <span data-ttu-id="28a8b-105">多層認知類神經網路的詳細討論是在此文件集的範圍之外。</span><span class="sxs-lookup"><span data-stu-id="28a8b-105">A detailed discussion of Multilayer Perceptron neural networks is outside the scope of this documentation.</span></span> <span data-ttu-id="28a8b-106">本主題說明演算法的基本實作，包括將輸入和輸出值正規化所使用的方法，以及減少屬性基數所使用的特徵選取方法。</span><span class="sxs-lookup"><span data-stu-id="28a8b-106">This topic explains the basic implementation of the algorithm, including the method used to normalize input and output values, and feature selection methods used to reduce attribute cardinality.</span></span> <span data-ttu-id="28a8b-107">本主題描述自訂演算法行為所使用的參數及其他設定，並提供關於查詢模型之其他資訊的連結。</span><span class="sxs-lookup"><span data-stu-id="28a8b-107">This topic describes the parameters and other settings that can be used to customize the behavior of the algorithm, and provides links to additional information about querying the model.</span></span>  
  
## <a name="implementation-of-the-microsoft-neural-network-algorithm"></a><span data-ttu-id="28a8b-108">Microsoft 類神經網路演算法的實作</span><span class="sxs-lookup"><span data-stu-id="28a8b-108">Implementation of the Microsoft Neural Network Algorithm</span></span>  
 <span data-ttu-id="28a8b-109">在多層認知類神經網路上，每一層神經會接收一或多個輸入，並會產生一或多個相同的輸出。</span><span class="sxs-lookup"><span data-stu-id="28a8b-109">In a Multilayer Perceptron neural network, each neuron receives one or more inputs and produces one or more identical outputs.</span></span> <span data-ttu-id="28a8b-110">每一個輸出就是神經輸入總和的簡易非線性函數。</span><span class="sxs-lookup"><span data-stu-id="28a8b-110">Each output is a simple non-linear function of the sum of the inputs to the neuron.</span></span> <span data-ttu-id="28a8b-111">輸入會從輸入層的節點向前傳遞到隱藏層的節點，然後再從隱藏層傳遞至輸出層；同一層內的神經之間沒有連接</span><span class="sxs-lookup"><span data-stu-id="28a8b-111">Inputs pass forward from nodes in the input layer to nodes in the hidden layer, and then pass from the hidden layer to the output layer; there are no connections between neurons within a layer.</span></span> <span data-ttu-id="28a8b-112">如果不包括隱藏層，則輸入會直接從輸入層的節點向前傳遞到輸出層的節點中，就像在羅吉斯迴歸模型一樣。</span><span class="sxs-lookup"><span data-stu-id="28a8b-112">If no hidden layer is included, as in a logistic regression model, inputs pass forward directly from nodes in the input layer to nodes in the output layer.</span></span>  
  
 <span data-ttu-id="28a8b-113">在以 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 類神經網路演算法建立的類神經網路中，有 3 種神經類型：</span><span class="sxs-lookup"><span data-stu-id="28a8b-113">There are three types of neurons in a neural network that is created with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm:</span></span>  
  
-   `Input neurons`  
  
 <span data-ttu-id="28a8b-114">輸入神經會提供資料採礦模型的輸入屬性值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-114">Input neurons provide input attribute values for the data mining model.</span></span> <span data-ttu-id="28a8b-115">針對離散輸入屬性，輸入神經通常代表輸入屬性的單一狀態。</span><span class="sxs-lookup"><span data-stu-id="28a8b-115">For discrete input attributes, an input neuron typically represents a single state from the input attribute.</span></span> <span data-ttu-id="28a8b-116">如果該屬性的定型資料包含 Null 值，這就包含遺漏值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-116">This includes missing values, if the training data contains nulls for that attribute.</span></span> <span data-ttu-id="28a8b-117">有兩個以上之狀態的分散輸入屬性會為每一個狀態產生一個輸入神經，如果定型資料中有任何 Null 值，則為遺漏狀態產生一個輸入神經。</span><span class="sxs-lookup"><span data-stu-id="28a8b-117">A discrete input attribute that has more than two states generates one input neuron for each state, and one input neuron for a missing state, if there are any nulls in the training data.</span></span> <span data-ttu-id="28a8b-118">連續輸入屬性會產生兩個輸入神經：一個神經用於遺漏狀態，而另一個神經用於連續屬性本身的值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-118">A continuous input attribute generates two input neurons: one neuron for a missing state, and one neuron for the value of the continuous attribute itself.</span></span> <span data-ttu-id="28a8b-119">輸入神經會提供輸入給一或多個隱藏神經。</span><span class="sxs-lookup"><span data-stu-id="28a8b-119">Input neurons provide inputs to one or more hidden neurons.</span></span>  
  
-   `Hidden neurons`  
  
 <span data-ttu-id="28a8b-120">隱藏神經會接收來自輸入神經的輸入，並提供輸出給輸出神經。</span><span class="sxs-lookup"><span data-stu-id="28a8b-120">Hidden neurons receive inputs from input neurons and provide outputs to output neurons.</span></span>  
  
-   `Output neurons`  
  
 <span data-ttu-id="28a8b-121">輸出神經代表資料採礦模型的可預測屬性值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-121">Output neurons represent predictable attribute values for the data mining model.</span></span> <span data-ttu-id="28a8b-122">針對分隔輸入屬性，輸出神經通常代表可預測屬性的單一預測狀態，包括遺漏值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-122">For discrete input attributes, an output neuron typically represents a single predicted state for a predictable attribute, including missing values.</span></span> <span data-ttu-id="28a8b-123">例如，二進位可預測屬性會產生一個描述遺漏狀態或現有狀態的輸出節點，指出該屬性的值是否存在。</span><span class="sxs-lookup"><span data-stu-id="28a8b-123">For example, a binary predictable attribute produces one output node that describes a missing or existing state, to indicate whether a value exists for that attribute.</span></span> <span data-ttu-id="28a8b-124">用來做為可預測屬性的布林資料行會產生 3 個輸出神經：一個用於 True 值的神經，一個用於 False 值的神經，以及一個用於遺漏狀態或現有狀態的神經。</span><span class="sxs-lookup"><span data-stu-id="28a8b-124">A Boolean column that is used as a predictable attribute generates three output neurons: one neuron for a true value, one neuron for a false value, and one neuron for a missing or existing state.</span></span> <span data-ttu-id="28a8b-125">有兩個以上之狀態的分隔可預測屬性會為每一個狀態產生一個輸出神經，並為遺漏狀態或現有狀態產生一個輸出神經。</span><span class="sxs-lookup"><span data-stu-id="28a8b-125">A discrete predictable attribute that has more than two states generates one output neuron for each state, and one output neuron for a missing or existing state.</span></span> <span data-ttu-id="28a8b-126">連續可預測資料行會產生兩個輸出神經：一個用於遺漏狀態或現有狀態的神經，一個用於連續資料行值本身的神經。</span><span class="sxs-lookup"><span data-stu-id="28a8b-126">Continuous predictable columns generate two output neurons: one neuron for a missing or existing state, and one neuron for the value of the continuous column itself.</span></span> <span data-ttu-id="28a8b-127">如果透過檢閱可預測資料行集產生的輸出神經超過 500 個， [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 就會在採礦模型中產生新網路來代表其他輸出神經。</span><span class="sxs-lookup"><span data-stu-id="28a8b-127">If more than 500 output neurons are generated by reviewing the set of predictable columns, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] generates a new network in the mining model to represent the additional output neurons.</span></span>  
  
 <span data-ttu-id="28a8b-128">神經會接收其他神經或其他資料的輸入，端視該神經位於網路的哪一層而定。</span><span class="sxs-lookup"><span data-stu-id="28a8b-128">A neuron receives input from other neurons, or from other data, depending on which layer of the network it is in.</span></span> <span data-ttu-id="28a8b-129">輸入神經會接收來自原始資料的輸入。</span><span class="sxs-lookup"><span data-stu-id="28a8b-129">An input neuron receives inputs from the original data.</span></span> <span data-ttu-id="28a8b-130">隱藏神經和輸出神經會接收類神經網路中，來自其他神經輸出的輸入。</span><span class="sxs-lookup"><span data-stu-id="28a8b-130">Hidden neurons and output neurons receive inputs from the output of other neurons in the neural network.</span></span> <span data-ttu-id="28a8b-131">輸入會建立神經之間的關聯性，而關聯性會做為特定案例集分析的路徑。</span><span class="sxs-lookup"><span data-stu-id="28a8b-131">Inputs establish relationships between neurons, and the relationships serve as a path of analysis for a specific set of cases.</span></span>  
  
 <span data-ttu-id="28a8b-132">每一個輸入都有被指派一個值，稱為 *「加權」*(Weight)，它描述該特定輸入對隱藏神經或輸出神經的相關性或重要性。</span><span class="sxs-lookup"><span data-stu-id="28a8b-132">Each input has a value assigned to it, called the *weight*, which describes the relevance or importance of that particular input to the hidden neuron or the output neuron.</span></span> <span data-ttu-id="28a8b-133">指派給輸入的加權越大，該輸入之值的相關性或重要性就越大。</span><span class="sxs-lookup"><span data-stu-id="28a8b-133">The greater the weight that is assigned to an input, the more relevant or important the value of that input.</span></span> <span data-ttu-id="28a8b-134">加權可以是負數，這表示輸入可以禁止而非啟動特定神經。</span><span class="sxs-lookup"><span data-stu-id="28a8b-134">Weights can be negative, which implies that the input can inhibit, rather than activate, a specific neuron.</span></span> <span data-ttu-id="28a8b-135">每個輸入的值會乘以加權來強調特定神經之輸入的重要性。</span><span class="sxs-lookup"><span data-stu-id="28a8b-135">The value of each input is multiplied by the weight to emphasize the importance of an input for a specific neuron.</span></span> <span data-ttu-id="28a8b-136">若為負加權，值乘以加權的結果表示不強調重要性。</span><span class="sxs-lookup"><span data-stu-id="28a8b-136">For negative weights, the effect of multiplying the value by the weight is to deemphasize the importance.</span></span>  
  
 <span data-ttu-id="28a8b-137">每一個神經都有被指派一個簡易非線性函數，稱為「啟用函數」\*\*，它描述特定神經對該類神經網路層的相關性或重要性。</span><span class="sxs-lookup"><span data-stu-id="28a8b-137">Each neuron has a simple non-linear function assigned to it, called the *activation function*, which describes the relevance or importance of a particular neuron to that layer of a neural network.</span></span> <span data-ttu-id="28a8b-138">隱藏神經會對其啟用函數使用「雙曲線正切」\*\* 函數 (tanh)，而輸出神經則會對其啟用函數使用「S 型」\*\*(sigmoid) 函數。</span><span class="sxs-lookup"><span data-stu-id="28a8b-138">Hidden neurons use a *hyperbolic tangent* function (tanh) for their activation function, whereas output neurons use a *sigmoid* function for activation.</span></span> <span data-ttu-id="28a8b-139">兩個函數都是非線性連續函數，可讓類神經網路建立輸入和輸出神經之間的非線性關聯性模型。</span><span class="sxs-lookup"><span data-stu-id="28a8b-139">Both functions are nonlinear, continuous functions that allow the neural network to model nonlinear relationships between input and output neurons.</span></span>  
  
### <a name="training-neural-networks"></a><span data-ttu-id="28a8b-140">培訓類神經網路</span><span class="sxs-lookup"><span data-stu-id="28a8b-140">Training Neural Networks</span></span>  
 <span data-ttu-id="28a8b-141">培訓使用 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 類神經網路演算法的資料採礦模型，包括數個步驟。</span><span class="sxs-lookup"><span data-stu-id="28a8b-141">Several steps are involved in training a data mining model that uses the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="28a8b-142">您針對演算法參數所指定的值，對這些步驟會有很大的影響。</span><span class="sxs-lookup"><span data-stu-id="28a8b-142">These steps are heavily influenced by the values that you specify for the algorithm parameters.</span></span>  
  
 <span data-ttu-id="28a8b-143">演算法會先從資料來源評估和擷取定型資料。</span><span class="sxs-lookup"><span data-stu-id="28a8b-143">The algorithm first evaluates and extracts training data from the data source.</span></span> <span data-ttu-id="28a8b-144">定型資料的百分比稱為 *「鑑效組資料」*(Holdout Data)，保留供評估網路的精確度之用。</span><span class="sxs-lookup"><span data-stu-id="28a8b-144">A percentage of the training data, called the *holdout data*, is reserved for use in assessing the accuracy of the network.</span></span> <span data-ttu-id="28a8b-145">在整個定型程序期間，定型資料的每一次反覆之後，網路就會立即接受評估。</span><span class="sxs-lookup"><span data-stu-id="28a8b-145">Throughout the training process, the network is evaluated immediately after each iteration through the training data.</span></span> <span data-ttu-id="28a8b-146">當精確度不再增加時，定型程序即停止。</span><span class="sxs-lookup"><span data-stu-id="28a8b-146">When the accuracy no longer increases, the training process is stopped.</span></span>  
  
 <span data-ttu-id="28a8b-147">*SAMPLE_SIZE* 和 *HOLDOUT_PERCENTAGE* 參數的值用於決定從定型資料取樣的案例數目，以及針對鑑效組資料暫擱的案例數目。</span><span class="sxs-lookup"><span data-stu-id="28a8b-147">The values of the *SAMPLE_SIZE* and *HOLDOUT_PERCENTAGE* parameters are used to determine the number of cases to sample from the training data and the number of cases to be put aside for the holdout data.</span></span> <span data-ttu-id="28a8b-148">*HOLDOUT_SEED* 參數的值用於隨機決定針對鑑效組資料而暫擱的個別案例。</span><span class="sxs-lookup"><span data-stu-id="28a8b-148">The value of the *HOLDOUT_SEED* parameter is used to randomly determine the individual cases to be put aside for the holdout data.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="28a8b-149">這些演算法參數與 HOLDOUT_SIZE 和 HOLDOUT_SEED 屬性不同，後兩者適用於採礦結構，藉以定義測試資料集。</span><span class="sxs-lookup"><span data-stu-id="28a8b-149">These algorithm parameters are different from the HOLDOUT_SIZE and HOLDOUT_SEED properties, which are applied to a mining structure to define a testing data set.</span></span>  
  
 <span data-ttu-id="28a8b-150">接下來，演算法會決定採礦模型支援的網路數目和複雜度。</span><span class="sxs-lookup"><span data-stu-id="28a8b-150">The algorithm next determines the number and complexity of the networks that the mining model supports.</span></span> <span data-ttu-id="28a8b-151">如果採礦模型包含一或多個只用於預測的屬性，則演算法會建立單一網路來代表所有這些屬性。</span><span class="sxs-lookup"><span data-stu-id="28a8b-151">If the mining model contains one or more attributes that are used only for prediction, the algorithm creates a single network that represents all such attributes.</span></span> <span data-ttu-id="28a8b-152">如果採礦模型包含一或多個同時用於輸入和預測的屬性，則演算法提供者會為每一個屬性建構網路。</span><span class="sxs-lookup"><span data-stu-id="28a8b-152">If the mining model contains one or more attributes that are used for both input and prediction, the algorithm provider constructs a network for each attribute.</span></span>  
  
 <span data-ttu-id="28a8b-153">針對具有離散值的輸入屬性和可預測屬性，每一個輸入或輸出神經分別代表單一狀態。</span><span class="sxs-lookup"><span data-stu-id="28a8b-153">For input and predictable attributes that have discrete values, each input or output neuron respectively represents a single state.</span></span> <span data-ttu-id="28a8b-154">針對具有連續值的輸入屬性和可預測屬性，每一個輸入或輸出神經分別代表該屬性值的範圍和散發。</span><span class="sxs-lookup"><span data-stu-id="28a8b-154">For input and predictable attributes that have continuous values, each input or output neuron respectively represents the range and distribution of values for the attribute.</span></span> <span data-ttu-id="28a8b-155">在任一案例中所支援的最大狀態數目，會視 *MAXIMUM_STATES* 演算法參數的值而定。</span><span class="sxs-lookup"><span data-stu-id="28a8b-155">The maximum number of states that is supported in either case depends on the value of the *MAXIMUM_STATES* algorithm parameter.</span></span> <span data-ttu-id="28a8b-156">如果特定屬性的狀態數目超過 *MAXIMUM_STATES* 演算法參數的值，則會選擇該屬性最常用或最相關的狀態 (不超過允許的最大狀態數目)，而其餘狀態則會分成遺漏值群組，作為分析用途。</span><span class="sxs-lookup"><span data-stu-id="28a8b-156">If the number of states for a specific attribute exceeds the value of the *MAXIMUM_STATES* algorithm parameter, the most popular or relevant states for that attribute are chosen, up to the maximum number of states allowed, and the remaining states are grouped as missing values for the purposes of analysis.</span></span>  
  
 <span data-ttu-id="28a8b-157">然後，決定要為隱藏層建立的初始神經數目時，演算法會使用 *HIDDEN_NODE_RATIO* 參數的值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-157">The algorithm then uses the value of the *HIDDEN_NODE_RATIO* parameter when determining the initial number of neurons to create for the hidden layer.</span></span> <span data-ttu-id="28a8b-158">您可以將 *HIDDEN_NODE_RATIO* 設定為 0，以防止在網路中建立演算法為採礦模型所產生的隱藏層，而將類神經網路視為邏輯迴歸來處理。</span><span class="sxs-lookup"><span data-stu-id="28a8b-158">You can set *HIDDEN_NODE_RATIO* to 0 to prevent the creation of a hidden layer in the networks that the algorithm generates for the mining model, to treat the neural network as a logistic regression.</span></span>  
  
 <span data-ttu-id="28a8b-159">演算法提供者反覆地同時評估網路上所有輸入的加權，使用先前保留的培訓資料集，以及將鑑效組資料中每一個案例的實際已知值與網路的預測做比較，這種程序稱為 *批次學習*。</span><span class="sxs-lookup"><span data-stu-id="28a8b-159">The algorithm provider iteratively evaluates the weight for all inputs across the network at the same time, by taking the set of training data that was reserved earlier and comparing the actual known value for each case in the holdout data with the network's prediction, in a process known as *batch learning*.</span></span> <span data-ttu-id="28a8b-160">在演算法評估整個培訓資料集之後，演算法會檢閱每一個神經的預測值和實際值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-160">After the algorithm has evaluated the entire set of training data, the algorithm reviews the predicted and actual value for each neuron.</span></span> <span data-ttu-id="28a8b-161">演算法會計算錯誤的程度 (如果有的話) 並調整與該神經的輸入相關聯的加權，從輸出神經回溯到輸入神經，這種程序稱為 *倒傳播*。</span><span class="sxs-lookup"><span data-stu-id="28a8b-161">The algorithm calculates the degree of error, if any, and adjusts the weights that are associated with the inputs for that neuron, working backward from output neurons to input neurons in a process known as *backpropagation*.</span></span> <span data-ttu-id="28a8b-162">接著，演算法會對整個培訓資料集重複此程序。</span><span class="sxs-lookup"><span data-stu-id="28a8b-162">The algorithm then repeats the process over the entire set of training data.</span></span> <span data-ttu-id="28a8b-163">因為演算法可以支援許多加權和輸出神經，所以使用結合漸層演算法 (conjugate gradient algorithm) 來引導培訓程序，以指派和評估輸入的加權。</span><span class="sxs-lookup"><span data-stu-id="28a8b-163">Because the algorithm can support many weights and output neurons, the conjugate gradient algorithm is used to guide the training process for assigning and evaluating weights for inputs.</span></span> <span data-ttu-id="28a8b-164">結合漸層演算法的討論超出此文件集的範圍之外。</span><span class="sxs-lookup"><span data-stu-id="28a8b-164">A discussion of the conjugate gradient algorithm is outside the scope of this documentation.</span></span>  
  
### <a name="feature-selection"></a><span data-ttu-id="28a8b-165">特徵選取</span><span class="sxs-lookup"><span data-stu-id="28a8b-165">Feature Selection</span></span>  
 <span data-ttu-id="28a8b-166">如果輸入屬性的數目大於 *MAXIMUM_INPUT_ATTRIBUTES* 參數的值，或者如果可預測屬性的數目大於 *MAXIMUM_OUTPUT_ATTRIBUTES* 參數的值，則會使用特徵選取演算法來降低包含在採礦模型中之網路的複雜度。</span><span class="sxs-lookup"><span data-stu-id="28a8b-166">If the number of input attributes is greater than the value of the *MAXIMUM_INPUT_ATTRIBUTES* parameter, or if the number of predictable attributes is greater than the value of the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, a feature selection algorithm is used to reduce the complexity of the networks that are included in the mining model.</span></span> <span data-ttu-id="28a8b-167">特徵選取會將輸入屬性或可預測屬性的數目減少為在統計上與模型最相關的數目。</span><span class="sxs-lookup"><span data-stu-id="28a8b-167">Feature selection reduces the number of input or predictable attributes to those that are most statistically relevant to the model.</span></span>  
  
 <span data-ttu-id="28a8b-168">所有 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 資料採礦演算法都會自動使用特徵選取來改善分析並減少處理的負載。</span><span class="sxs-lookup"><span data-stu-id="28a8b-168">Feature selection is used automatically by all [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms to improve analysis and reduce processing load.</span></span> <span data-ttu-id="28a8b-169">在類神經網路模型中，特徵選取所使用的方法取決於屬性的資料類型。</span><span class="sxs-lookup"><span data-stu-id="28a8b-169">The method used for feature selection in neural network models depends on the data type of the attribute.</span></span> <span data-ttu-id="28a8b-170">下表顯示用於類神經網路模型的特徵選取方法，並同時顯示用於羅吉斯迴歸演算法 (以類神經網路演算法為基礎) 的特徵選取方法，做為參考。</span><span class="sxs-lookup"><span data-stu-id="28a8b-170">For reference, the following table shows the feature selection methods used for neural network models, and also shows the feature selection methods used for the Logistic Regression algorithm, which is based on the Neural Network algorithm.</span></span>  
  
|<span data-ttu-id="28a8b-171">演算法</span><span class="sxs-lookup"><span data-stu-id="28a8b-171">Algorithm</span></span>|<span data-ttu-id="28a8b-172">分析的方法</span><span class="sxs-lookup"><span data-stu-id="28a8b-172">Method of analysis</span></span>|<span data-ttu-id="28a8b-173">註解</span><span class="sxs-lookup"><span data-stu-id="28a8b-173">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="28a8b-174">類神經網路</span><span class="sxs-lookup"><span data-stu-id="28a8b-174">Neural Network</span></span>|<span data-ttu-id="28a8b-175">有趣性分數</span><span class="sxs-lookup"><span data-stu-id="28a8b-175">Interestingness score</span></span><br /><br /> <span data-ttu-id="28a8b-176">Shannon 熵</span><span class="sxs-lookup"><span data-stu-id="28a8b-176">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="28a8b-177">使用 K2 優先的貝氏</span><span class="sxs-lookup"><span data-stu-id="28a8b-177">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="28a8b-178">使用優先統一狄氏分配的貝氏 (預設值)</span><span class="sxs-lookup"><span data-stu-id="28a8b-178">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="28a8b-179">類神經網路演算法可以使用以熵為基礎的計分方法和貝氏計分方法，只要資料包含連續資料行即可。</span><span class="sxs-lookup"><span data-stu-id="28a8b-179">The Neural Networks algorithm can use both entropy-based and Bayesian scoring methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="28a8b-180">預設值：</span><span class="sxs-lookup"><span data-stu-id="28a8b-180">Default.</span></span>|  
|<span data-ttu-id="28a8b-181">羅吉斯迴歸</span><span class="sxs-lookup"><span data-stu-id="28a8b-181">Logistic Regression</span></span>|<span data-ttu-id="28a8b-182">有趣性分數</span><span class="sxs-lookup"><span data-stu-id="28a8b-182">Interestingness score</span></span><br /><br /> <span data-ttu-id="28a8b-183">Shannon 熵</span><span class="sxs-lookup"><span data-stu-id="28a8b-183">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="28a8b-184">使用 K2 優先的貝氏</span><span class="sxs-lookup"><span data-stu-id="28a8b-184">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="28a8b-185">使用優先統一狄氏分配的貝氏 (預設值)</span><span class="sxs-lookup"><span data-stu-id="28a8b-185">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="28a8b-186">因為您無法將參數傳遞給這個演算法來控制特徵選取行為，所以會使用預設值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-186">Because you cannot pass a parameter to this algorithm to control feature election behavior, the defaults are used.</span></span> <span data-ttu-id="28a8b-187">因此，如果所有屬性都是離散或離散化的，則預設值為 BDEU。</span><span class="sxs-lookup"><span data-stu-id="28a8b-187">Therefore, if all attributes are discrete or discretized, the default is BDEU.</span></span>|  
  
 <span data-ttu-id="28a8b-188">針對類神經網路模型控制特徵選取的演算法參數為 MAXIMUM_INPUT_ATTRIBUTES、MAXIMUM_OUTPUT_ATTRIBUTES 和 MAXIMUM_STATES。</span><span class="sxs-lookup"><span data-stu-id="28a8b-188">The algorithm parameters that control feature selection for a neural network model are MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES, and MAXIMUM_STATES.</span></span> <span data-ttu-id="28a8b-189">您也可以設定 HIDDEN_NODE_RATIO 參數，以控制隱藏層的數目。</span><span class="sxs-lookup"><span data-stu-id="28a8b-189">You can also control the number of hidden layers by setting the HIDDEN_NODE_RATIO parameter.</span></span>  
  
### <a name="scoring-methods"></a><span data-ttu-id="28a8b-190">計分方法</span><span class="sxs-lookup"><span data-stu-id="28a8b-190">Scoring Methods</span></span>  
 <span data-ttu-id="28a8b-191">*「計分」* (Scoring) 是一種正規化，在定型類神經網路模型的內容中表示一種程序，會將離散文字標籤之類的值轉換為可以與網路中其他輸入類型比較並加權的值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-191">*Scoring* is a kind of normalization, which in the context of training a neural network model means the process of converting a value, such as a discrete text label, into a value that can be compared with other types of inputs and weighted in the network.</span></span> <span data-ttu-id="28a8b-192">例如，如果其中一個輸入屬性為 Gender，且可能的值為 Male 和 Female，而另一個輸入屬性為 Income，且包含值的變數範圍，則每個屬性的值無法直接比較，因此必須編碼為一般小數位數，讓加權可以計算。</span><span class="sxs-lookup"><span data-stu-id="28a8b-192">For example, if one input attribute is Gender and the possible values are Male and Female, and another input attribute is Income, with a variable range of values, the values for each attribute are not directly comparable, and therefore must be encoded to a common scale so that the weights can be computed.</span></span> <span data-ttu-id="28a8b-193">計分是將此類輸入正規化為數值的程序：特別是正規化為機率範圍。</span><span class="sxs-lookup"><span data-stu-id="28a8b-193">Scoring is the process of normalizing such inputs to numeric values: specifically, to a probability range.</span></span> <span data-ttu-id="28a8b-194">用於正規化的函數也有助於針對統一的比例，更平均地散發輸入值，如此一來，極端值就不會扭曲分析的結果。</span><span class="sxs-lookup"><span data-stu-id="28a8b-194">The functions used for normalization also help to distribute input value more evenly on a uniform scale so that extreme values do not distort the results of analysis.</span></span>  
  
 <span data-ttu-id="28a8b-195">類神經網路的輸出也會經過編碼。</span><span class="sxs-lookup"><span data-stu-id="28a8b-195">Outputs of the neural network are also encoded.</span></span> <span data-ttu-id="28a8b-196">當輸出 (也就是預測) 有一個單一目標，或有僅用於預測，而不用於輸入的多個目標時，模型會建立一個單一網路，而且它可能不需要將值正規化。</span><span class="sxs-lookup"><span data-stu-id="28a8b-196">When there is a single target for output (that is, prediction), or multiple targets that are used for prediction only and not for input, the model create a single network and it might not seem necessary to normalize the values.</span></span> <span data-ttu-id="28a8b-197">不過，如果輸入和預測使用多個屬性，模型必須建立多個網路，因此，所有值都必須正規化，而且在離開網路時，也必須將輸出編碼。</span><span class="sxs-lookup"><span data-stu-id="28a8b-197">However, if multiple attributes are used for input and prediction, the model must create multiple networks; therefore, all values must be normalized, and the outputs too must be encoded as they exit the network.</span></span>  
  
 <span data-ttu-id="28a8b-198">輸入的編碼是以定型案例中每個離散值的總和為基礎，然後將該值乘以其加權。</span><span class="sxs-lookup"><span data-stu-id="28a8b-198">Encoding for inputs is based on summing each discrete value in the training cases, and multiplying that value by its weight.</span></span> <span data-ttu-id="28a8b-199">這就是所謂的 *「加權總和」*(Weighted Sum)，這個加權總和會傳遞到隱藏層中的啟用函數。</span><span class="sxs-lookup"><span data-stu-id="28a8b-199">This is called a *weighted sum*, which is passed to the activation function in the hidden layer.</span></span> <span data-ttu-id="28a8b-200">Z-score 用於編碼，如下所示：</span><span class="sxs-lookup"><span data-stu-id="28a8b-200">A z-score is used for encoding, as follows:</span></span>  
  
 <span data-ttu-id="28a8b-201">**離散值**</span><span class="sxs-lookup"><span data-stu-id="28a8b-201">**Discrete values**</span></span>  
  
 <span data-ttu-id="28a8b-202">μ = p-狀態的先前機率</span><span class="sxs-lookup"><span data-stu-id="28a8b-202">μ = p - the prior probability of a state</span></span>  
  
 <span data-ttu-id="28a8b-203">StdDev  = sqrt(p(1-p))</span><span class="sxs-lookup"><span data-stu-id="28a8b-203">StdDev  = sqrt(p(1-p))</span></span>  
  
 <span data-ttu-id="28a8b-204">**連續值**</span><span class="sxs-lookup"><span data-stu-id="28a8b-204">**Continuous values**</span></span>  
  
 <span data-ttu-id="28a8b-205">值存在 = 1-μ/σ</span><span class="sxs-lookup"><span data-stu-id="28a8b-205">Value present= 1 - μ/σ</span></span>  
  
 <span data-ttu-id="28a8b-206">無現有值 =-μ/σ</span><span class="sxs-lookup"><span data-stu-id="28a8b-206">No existing value= -μ/σ</span></span>  
  
 <span data-ttu-id="28a8b-207">這些值經過編碼之後，輸入會以網路邊緣當做加權，進行加權總和。</span><span class="sxs-lookup"><span data-stu-id="28a8b-207">After the values have been encoded, the inputs go through weighted summing, with network edges as weights.</span></span>  
  
 <span data-ttu-id="28a8b-208">輸出的編碼使用 sigmoid 函數，其中包含的屬性對於預測相當實用。</span><span class="sxs-lookup"><span data-stu-id="28a8b-208">Encoding for outputs uses the sigmoid function, which has properties that make it very useful for prediction.</span></span> <span data-ttu-id="28a8b-209">其中一個這種屬性是，不管如何調整原始值，也不管是負值還是正值，此函數的輸出永遠是介於 0 和 1 的值，也就是適合評估機率的值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-209">One such property is that, regardless of how the original values are scaled, and regardless of whether values are negative or positive, the output of this function is always a value between 0 and 1, which is suited for estimating probabilities.</span></span> <span data-ttu-id="28a8b-210">其他實用的屬性是，sigmoid 函數具備緩衝效果，因此，當這些值與轉折點的距離比較遠時，該值的機率就會向前移到 0 或 1，但是移動速度很緩慢。</span><span class="sxs-lookup"><span data-stu-id="28a8b-210">Another useful property is that the sigmoid function has a smoothing effect, so that as values move farther away from point of inflection, the probability for the value moves towards 0 or 1, but slowly.</span></span>  
  
## <a name="customizing-the-neural-network-algorithm"></a><span data-ttu-id="28a8b-211">自訂類神經網路演算法</span><span class="sxs-lookup"><span data-stu-id="28a8b-211">Customizing the Neural Network Algorithm</span></span>  
 <span data-ttu-id="28a8b-212">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 類神經網路演算法支援數個會影響所產生之採礦模型的行為、效能和精確度的參數。</span><span class="sxs-lookup"><span data-stu-id="28a8b-212">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports several parameters that affect the behavior, performance, and accuracy of the resulting mining model.</span></span> <span data-ttu-id="28a8b-213">您也可以在資料行上設定模型旗標，或設定散發旗標來指定如何處理資料行內的值，藉以修改模型處理資料的方式。</span><span class="sxs-lookup"><span data-stu-id="28a8b-213">You can also modify the way that the model processes data by setting modeling flags on columns, or by setting distribution flags to specify how values within the column are handled.</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="28a8b-214">設定演算法參數</span><span class="sxs-lookup"><span data-stu-id="28a8b-214">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="28a8b-215">下表描述可搭配 Microsoft 類神經網路演算法使用的參數。</span><span class="sxs-lookup"><span data-stu-id="28a8b-215">The following table describes the parameters that can be used with the Microsoft Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="28a8b-216">HIDDEN_NODE_RATIO</span><span class="sxs-lookup"><span data-stu-id="28a8b-216">HIDDEN_NODE_RATIO</span></span>  
 <span data-ttu-id="28a8b-217">指定隱藏神經與輸入和輸出神經的比例。</span><span class="sxs-lookup"><span data-stu-id="28a8b-217">Specifies the ratio of hidden neurons to input and output neurons.</span></span> <span data-ttu-id="28a8b-218">下列公式決定隱藏層中的初始神經數目：</span><span class="sxs-lookup"><span data-stu-id="28a8b-218">The following formula determines the initial number of neurons in the hidden layer:</span></span>  
  
 <span data-ttu-id="28a8b-219">HIDDEN_NODE_RATIO \* SQRT(總輸入神經 \* 總輸出神經)</span><span class="sxs-lookup"><span data-stu-id="28a8b-219">HIDDEN_NODE_RATIO \* SQRT(Total input neurons \* Total output neurons)</span></span>  
  
 <span data-ttu-id="28a8b-220">預設值為 4.0。</span><span class="sxs-lookup"><span data-stu-id="28a8b-220">The default value is 4.0.</span></span>  
  
 <span data-ttu-id="28a8b-221">HOLDOUT_PERCENTAGE</span><span class="sxs-lookup"><span data-stu-id="28a8b-221">HOLDOUT_PERCENTAGE</span></span>  
 <span data-ttu-id="28a8b-222">指定用來計算鑑效組錯誤之定型資料內的案例百分比，這可作為定型採礦模型時停止準則的一部分。</span><span class="sxs-lookup"><span data-stu-id="28a8b-222">Specifies the percentage of cases within the training data used to calculate the holdout error, which is used as part of the stopping criteria while training the mining model.</span></span>  
  
 <span data-ttu-id="28a8b-223">預設值是 30。</span><span class="sxs-lookup"><span data-stu-id="28a8b-223">The default value is 30.</span></span>  
  
 <span data-ttu-id="28a8b-224">HOLDOUT_SEED</span><span class="sxs-lookup"><span data-stu-id="28a8b-224">HOLDOUT_SEED</span></span>  
 <span data-ttu-id="28a8b-225">在演算法隨機決定鑑效組資料時，指定用來植入虛擬隨機產生器的數字。</span><span class="sxs-lookup"><span data-stu-id="28a8b-225">Specifies a number that is used to seed the pseudo-random generator when the algorithm randomly determines the holdout data.</span></span> <span data-ttu-id="28a8b-226">如果此參數設定為 0，此演算法會依據採礦模型的名稱產生種子，以保證在重新處理期間，模型內容保持不變。</span><span class="sxs-lookup"><span data-stu-id="28a8b-226">If this parameter is set to 0, the algorithm generates the seed based on the name of the mining model, to guarantee that the model content remains the same during reprocessing.</span></span>  
  
 <span data-ttu-id="28a8b-227">預設值為 0。</span><span class="sxs-lookup"><span data-stu-id="28a8b-227">The default value is 0.</span></span>  
  
 <span data-ttu-id="28a8b-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="28a8b-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="28a8b-229">決定在運用特徵選取之前可提供給演算法之輸入屬性的最大數目。</span><span class="sxs-lookup"><span data-stu-id="28a8b-229">Determines the maximum number of input attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="28a8b-230">將此值設定為 0，會停用輸入屬性的特徵選取。</span><span class="sxs-lookup"><span data-stu-id="28a8b-230">Setting this value to 0 disables feature selection for input attributes.</span></span>  
  
 <span data-ttu-id="28a8b-231">預設值為 255。</span><span class="sxs-lookup"><span data-stu-id="28a8b-231">The default value is 255.</span></span>  
  
 <span data-ttu-id="28a8b-232">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="28a8b-232">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="28a8b-233">決定在運用特徵選取之前可提供給演算法之輸出屬性的最大數目。</span><span class="sxs-lookup"><span data-stu-id="28a8b-233">Determines the maximum number of output attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="28a8b-234">將此值設定為 0，會停用輸出屬性的特徵選取。</span><span class="sxs-lookup"><span data-stu-id="28a8b-234">Setting this value to 0 disables feature selection for output attributes.</span></span>  
  
 <span data-ttu-id="28a8b-235">預設值為 255。</span><span class="sxs-lookup"><span data-stu-id="28a8b-235">The default value is 255.</span></span>  
  
 <span data-ttu-id="28a8b-236">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="28a8b-236">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="28a8b-237">指定演算法支援的每個屬性之離散狀態的最大數目。</span><span class="sxs-lookup"><span data-stu-id="28a8b-237">Specifies the maximum number of discrete states per attribute that is supported by the algorithm.</span></span> <span data-ttu-id="28a8b-238">如果特定屬性的狀態數目大於針對這個參數所指定的數字，則演算法會使用該屬性最常用的狀態，並將其餘狀態視為遺漏。</span><span class="sxs-lookup"><span data-stu-id="28a8b-238">If the number of states for a specific attribute is greater than the number that is specified for this parameter, the algorithm uses the most popular states for that attribute and treats the remaining states as missing.</span></span>  
  
 <span data-ttu-id="28a8b-239">預設值是 100。</span><span class="sxs-lookup"><span data-stu-id="28a8b-239">The default value is 100.</span></span>  
  
 <span data-ttu-id="28a8b-240">SAMPLE_SIZE</span><span class="sxs-lookup"><span data-stu-id="28a8b-240">SAMPLE_SIZE</span></span>  
 <span data-ttu-id="28a8b-241">指定用來定型模型的案例數目。</span><span class="sxs-lookup"><span data-stu-id="28a8b-241">Specifies the number of cases to be used to train the model.</span></span> <span data-ttu-id="28a8b-242">此演算法會使用此數字或不包括在鑑效組資料中之總案例數的百分比 (由 HOLDOUT_PERCENTAGE 參數指定)，以較小者為準。</span><span class="sxs-lookup"><span data-stu-id="28a8b-242">The algorithm uses either this number or the percentage of total of cases not included in the holdout data as specified by the HOLDOUT_PERCENTAGE parameter, whichever value is smaller.</span></span>  
  
 <span data-ttu-id="28a8b-243">換句話說，如果 HOLDOUT_PERCENTAGE 設定為 30，則演算法將使用這個參數的值或等於總案例數 70% 的值，以較小者為準。</span><span class="sxs-lookup"><span data-stu-id="28a8b-243">In other words, if HOLDOUT_PERCENTAGE is set to 30, the algorithm will use either the value of this parameter, or a value equal to 70 percent of the total number of cases, whichever is smaller.</span></span>  
  
 <span data-ttu-id="28a8b-244">預設值為 10000。</span><span class="sxs-lookup"><span data-stu-id="28a8b-244">The default value is 10000.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="28a8b-245">模型旗標</span><span class="sxs-lookup"><span data-stu-id="28a8b-245">Modeling Flags</span></span>  
 <span data-ttu-id="28a8b-246">系統支援下列模型旗標，可搭配 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 類神經網路演算法使用。</span><span class="sxs-lookup"><span data-stu-id="28a8b-246">The following modeling flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="28a8b-247">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="28a8b-247">NOT NULL</span></span>  
 <span data-ttu-id="28a8b-248">表示資料行不能包含 Null 值。</span><span class="sxs-lookup"><span data-stu-id="28a8b-248">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="28a8b-249">如果 Analysis Services 在模型定型期間遇到 Null 值，將會產生錯誤。</span><span class="sxs-lookup"><span data-stu-id="28a8b-249">An error will result if Analysis Services encounters a null during model training.</span></span>  
  
 <span data-ttu-id="28a8b-250">適用於採礦結構資料行。</span><span class="sxs-lookup"><span data-stu-id="28a8b-250">Applies to mining structure columns.</span></span>  
  
 <span data-ttu-id="28a8b-251">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="28a8b-251">MODEL_EXISTENCE_ONLY</span></span>  
 <span data-ttu-id="28a8b-252">表示模型應該只考量屬性的值是否存在，或者值是否遺漏。</span><span class="sxs-lookup"><span data-stu-id="28a8b-252">Indicates that the model should only consider whether a value exists for the attribute or if a value is missing.</span></span> <span data-ttu-id="28a8b-253">確實的值不相符。</span><span class="sxs-lookup"><span data-stu-id="28a8b-253">The exact value does not matter.</span></span>  
  
 <span data-ttu-id="28a8b-254">適用於採礦模型資料行。</span><span class="sxs-lookup"><span data-stu-id="28a8b-254">Applies to mining model columns.</span></span>  
  
### <a name="distribution-flags"></a><span data-ttu-id="28a8b-255">散發旗標</span><span class="sxs-lookup"><span data-stu-id="28a8b-255">Distribution Flags</span></span>  
 <span data-ttu-id="28a8b-256">系統支援下列散發旗標，可搭配 [!INCLUDE[msCoName](../../includes/msconame-md.md)] 類神經網路演算法使用。</span><span class="sxs-lookup"><span data-stu-id="28a8b-256">The following distribution flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="28a8b-257">旗標只能當做模型的提示使用；如果演算法偵測到不同的散發，它將會使用找到的散發，而不是提示中提供的散發。</span><span class="sxs-lookup"><span data-stu-id="28a8b-257">The flags are used as hints to the model only; if the algorithm detects a different distribution it will use the found distribution, not the distribution provided in the hint.</span></span>  
  
 <span data-ttu-id="28a8b-258">正常</span><span class="sxs-lookup"><span data-stu-id="28a8b-258">Normal</span></span>  
 <span data-ttu-id="28a8b-259">表示資料行內的值應該被視為它們代表正常或高斯、散發。</span><span class="sxs-lookup"><span data-stu-id="28a8b-259">Indicates that values within the column should be treated as though they represent the normal, or Gaussian, distribution.</span></span>  
  
 <span data-ttu-id="28a8b-260">Uniform</span><span class="sxs-lookup"><span data-stu-id="28a8b-260">Uniform</span></span>  
 <span data-ttu-id="28a8b-261">表示資料行內的值應該被視為它們以統一的方式散發，也就是說，任何值的機率都大致相等，而且是值總數的函數。</span><span class="sxs-lookup"><span data-stu-id="28a8b-261">Indicates that values within the column should be treated as though they are distributed uniformly; that is, the probability of any value is roughly equal, and is a function of the total number of values.</span></span>  
  
 <span data-ttu-id="28a8b-262">Log Normal</span><span class="sxs-lookup"><span data-stu-id="28a8b-262">Log Normal</span></span>  
 <span data-ttu-id="28a8b-263">表示資料行內的值應視為根據「對數常態」\*\* 曲線分佈，也就是說值的對數會以常態方式分佈。</span><span class="sxs-lookup"><span data-stu-id="28a8b-263">Indicates that values within the column should be treated as though distributed according to the *log normal* curve, which means that the logarithm of the values is distributed normally.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="28a8b-264">需求</span><span class="sxs-lookup"><span data-stu-id="28a8b-264">Requirements</span></span>  
 <span data-ttu-id="28a8b-265">類神經網路模型必須至少包含一個輸入資料行和一個輸出資料行。</span><span class="sxs-lookup"><span data-stu-id="28a8b-265">A neural network model must contain at least one input column and one output column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="28a8b-266">輸入和可預測資料行</span><span class="sxs-lookup"><span data-stu-id="28a8b-266">Input and Predictable Columns</span></span>  
 <span data-ttu-id="28a8b-267">[!INCLUDE[msCoName](../../includes/msconame-md.md)] 類神經網路演算法支援下表所列的特定輸入資料行和可預測資料行。</span><span class="sxs-lookup"><span data-stu-id="28a8b-267">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span>  
  
|<span data-ttu-id="28a8b-268">資料行</span><span class="sxs-lookup"><span data-stu-id="28a8b-268">Column</span></span>|<span data-ttu-id="28a8b-269">內容類型</span><span class="sxs-lookup"><span data-stu-id="28a8b-269">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="28a8b-270">輸入屬性</span><span class="sxs-lookup"><span data-stu-id="28a8b-270">Input attribute</span></span>|<span data-ttu-id="28a8b-271">Continuous、Cyclical、Discrete、Discretized、Key、Table 和 Ordered</span><span class="sxs-lookup"><span data-stu-id="28a8b-271">Continuous, Cyclical, Discrete, Discretized, Key, Table, and Ordered</span></span>|  
|<span data-ttu-id="28a8b-272">可預測屬性</span><span class="sxs-lookup"><span data-stu-id="28a8b-272">Predictable attribute</span></span>|<span data-ttu-id="28a8b-273">Continuous、Cyclical、Discrete、Discretized 和 Ordered</span><span class="sxs-lookup"><span data-stu-id="28a8b-273">Continuous, Cyclical, Discrete, Discretized, and Ordered</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="28a8b-274">系統支援 Cyclical 和 Ordered 內容類型，但是演算法將它們視為離散值，因此不會執行特殊處理。</span><span class="sxs-lookup"><span data-stu-id="28a8b-274">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="28a8b-275">另請參閱</span><span class="sxs-lookup"><span data-stu-id="28a8b-275">See Also</span></span>  
 <span data-ttu-id="28a8b-276">[Microsoft 類神經網路演算法](microsoft-neural-network-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="28a8b-276">[Microsoft Neural Network Algorithm](microsoft-neural-network-algorithm.md) </span></span>  
 <span data-ttu-id="28a8b-277">[類神經網路模型的採礦模型內容 &#40;Analysis Services 資料採礦&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span><span class="sxs-lookup"><span data-stu-id="28a8b-277">[Mining Model Content for Neural Network Models &#40;Analysis Services - Data Mining&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span></span>  
 [<span data-ttu-id="28a8b-278">類神經網路模型查詢範例</span><span class="sxs-lookup"><span data-stu-id="28a8b-278">Neural Network Model Query Examples</span></span>](neural-network-model-query-examples.md)  
  
  
