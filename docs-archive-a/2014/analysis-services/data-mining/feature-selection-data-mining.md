---
title: " (資料採礦) 的特徵選取 |Microsoft Docs"
ms.custom: ''
ms.date: 03/06/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- mining models [Analysis Services], feature selections
- attributes [data mining]
- feature selection algorithms [Analysis Services]
- data mining [Analysis Services], feature selections
- neural network algorithms [Analysis Services]
- naive bayes algorithms [Analysis Services]
- decision tree algorithms [Analysis Services]
- datasets [Analysis Services]
- clustering algorithms [Analysis Services]
- coding [Data Mining]
ms.assetid: b044e785-4875-45ab-8ae4-cd3b4e3033bb
author: minewiskan
ms.author: owend
ms.openlocfilehash: ef5ee56636e7710074a893aed733905e1bf983bd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 08/04/2020
ms.locfileid: "87597455"
---
# <a name="feature-selection-data-mining"></a><span data-ttu-id="9016d-102">特徵選取 (資料採礦)</span><span class="sxs-lookup"><span data-stu-id="9016d-102">Feature Selection (Data Mining)</span></span>
  <span data-ttu-id="9016d-103">「*特徵選取」（Feature selection* ）一詞常用於資料採礦中，描述可用來將輸入縮減為可管理大小以進行處理和分析的工具和技術。</span><span class="sxs-lookup"><span data-stu-id="9016d-103">*Feature selection* is a term commonly used in data mining to describe the tools and techniques available for reducing inputs to a manageable size for processing and analysis.</span></span> <span data-ttu-id="9016d-104">特徵選取不僅意味著*基數縮減*，也就是在建立模型時可考慮的屬性數目施加任意或預先定義的截止，同時也會選擇屬性，這表示分析師或模型化工具會根據其對分析的實用性，主動選取或捨棄屬性。</span><span class="sxs-lookup"><span data-stu-id="9016d-104">Feature selection implies not only *cardinality reduction*, which means imposing an arbitrary or predefined cutoff on the number of attributes that can be considered when building a model, but also the choice of attributes, meaning that either the analyst or the modeling tool actively selects or discards attributes based on their usefulness for analysis.</span></span>  
  
 <span data-ttu-id="9016d-105">能夠套用特徵選取對有效分析而言很重要，因為資料集經常包含比建立模型所需還要更多的資訊。</span><span class="sxs-lookup"><span data-stu-id="9016d-105">The ability to apply feature selection is critical for effective analysis, because datasets frequently contain far more information than is needed to build the model.</span></span> <span data-ttu-id="9016d-106">例如，資料集可能包含 500 個資料行來描述客戶的特性，但是如果其中一些資料行的資料很疏鬆，將資料加入至模型所能獲得的效益將非常有限。</span><span class="sxs-lookup"><span data-stu-id="9016d-106">For example, a dataset might contain 500 columns that describe the characteristics of customers, but if the data in some of the columns is very sparse you would gain very little benefit from adding them to the model.</span></span> <span data-ttu-id="9016d-107">如果您在建立此模型時保留不需要的資料行，定型程序期間就會需要更多 CPU 和記憶體，而且完成的模型需要更多儲存空間。</span><span class="sxs-lookup"><span data-stu-id="9016d-107">If you keep the unneeded columns while building the model, more CPU and memory are required during the training process, and more storage space is required for the completed model.</span></span>  
  
 <span data-ttu-id="9016d-108">即使資源不是問題，您通常也會想要移除不需要的資料行，因為它們可能會降低已探索模式的品質，原因如下：</span><span class="sxs-lookup"><span data-stu-id="9016d-108">Even if resources are not an issue, you typically want to remove unneeded columns because they might degrade the quality of discovered patterns, for the following reasons:</span></span>  
  
-   <span data-ttu-id="9016d-109">某些資料行是累贅或多餘的。</span><span class="sxs-lookup"><span data-stu-id="9016d-109">Some columns are noisy or redundant.</span></span> <span data-ttu-id="9016d-110">這種情況會使有意義的資料模式更難發現。</span><span class="sxs-lookup"><span data-stu-id="9016d-110">This noise makes it more difficult to discover meaningful patterns from the data;</span></span>  
  
-   <span data-ttu-id="9016d-111">若要發現高品質的模式，大部分的資料採礦演算法需要高維度資料集提供大得多的訓練資料集。</span><span class="sxs-lookup"><span data-stu-id="9016d-111">To discover quality patterns, most data mining algorithms require much larger training data set on high-dimensional data set.</span></span> <span data-ttu-id="9016d-112">但在某些資料採礦應用程式中，定型資料的量非常少。</span><span class="sxs-lookup"><span data-stu-id="9016d-112">But the training data is very small in some data mining applications.</span></span>  
  
 <span data-ttu-id="9016d-113">如果資料來源的 500 個資料行中只有 50 個資料行的資訊對建立模型有用，您可以不將其納入模型，或使用特徵選取技術自動探索最佳的特徵，並排除統計上不重要的值。</span><span class="sxs-lookup"><span data-stu-id="9016d-113">If only 50 of the 500 columns in the data source have information that is useful in building a model, you could just leave them out of the model, or you could use feature selection techniques to automatically discover the best features and to exclude values that are statistically insignificant.</span></span> <span data-ttu-id="9016d-114">特徵選取有助於解決兩個問題：低價值資料過多，或高價值資料過少。</span><span class="sxs-lookup"><span data-stu-id="9016d-114">Feature selection helps solve the twin problems of having too much data that is of little value, or having too little data that is of high value.</span></span>  
  
## <a name="feature-selection-in-analysis-services-data-mining"></a><span data-ttu-id="9016d-115">Analysis Services 資料採礦中的特徵選取</span><span class="sxs-lookup"><span data-stu-id="9016d-115">Feature Selection in Analysis Services Data Mining</span></span>  
 <span data-ttu-id="9016d-116">特徵選取通常會在 [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] 中自動執行，且每個演算法具有一組預設的技術，可以有智慧地套用特徵減少。</span><span class="sxs-lookup"><span data-stu-id="9016d-116">Usually, feature selection is performed automatically in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], and each algorithm has a set of default techniques for intelligently applying feature reduction.</span></span> <span data-ttu-id="9016d-117">特徵選取一定會在定型模型之前執行，可自動從資料集裡選擇最有可能在模型中使用的屬性。</span><span class="sxs-lookup"><span data-stu-id="9016d-117">Feature selection is always performed before the model is trained, to automatically choose the attributes in a dataset that are most likely to be used in the model.</span></span> <span data-ttu-id="9016d-118">但是，您也可以手動設定參數以影響特徵選取行為。</span><span class="sxs-lookup"><span data-stu-id="9016d-118">However, you can also manually set parameters to influence feature selection behavior.</span></span>  
  
 <span data-ttu-id="9016d-119">一般而言，特徵選取的運作方式是計算每個屬性的分數，然後僅選取擁有最佳分數的屬性。</span><span class="sxs-lookup"><span data-stu-id="9016d-119">In general, feature selection works by calculating a score for each attribute, and then selecting only the attributes that have the best scores.</span></span> <span data-ttu-id="9016d-120">您也可以調整高分的臨界值。</span><span class="sxs-lookup"><span data-stu-id="9016d-120">You can also adjust the threshold for the top scores.</span></span> [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] <span data-ttu-id="9016d-121">提供多種可用於計算這些分數的方法，而實際套用至任何模型的方法則取決於下列因素：</span><span class="sxs-lookup"><span data-stu-id="9016d-121">provides multiple methods for calculating these scores, and the exact method that is applied in any model depends on these factors:</span></span>  
  
-   <span data-ttu-id="9016d-122">模型中所使用的演算法</span><span class="sxs-lookup"><span data-stu-id="9016d-122">The algorithm used in your model</span></span>  
  
-   <span data-ttu-id="9016d-123">屬性的資料類型</span><span class="sxs-lookup"><span data-stu-id="9016d-123">The data type of the attribute</span></span>  
  
-   <span data-ttu-id="9016d-124">您可能在模型上設定的任何參數</span><span class="sxs-lookup"><span data-stu-id="9016d-124">Any parameters that you may have set on your model</span></span>  
  
 <span data-ttu-id="9016d-125">特徵選取會套用到輸入、可預測的屬性或是資料行中的狀態。</span><span class="sxs-lookup"><span data-stu-id="9016d-125">Feature selection is applied to inputs, predictable attributes, or to states in a column.</span></span> <span data-ttu-id="9016d-126">完成特徵選取的計分之後，只有演算法選取的屬性和狀態會包含在模型建立程序中，而且可以用來預測。</span><span class="sxs-lookup"><span data-stu-id="9016d-126">When scoring for feature selection is complete, only the attributes and states that the algorithm selects are included in the model-building process and can be used for prediction.</span></span> <span data-ttu-id="9016d-127">如果您選擇的可預測屬性不符合特徵選取的臨界值，屬性仍可用於預測，但預測只會以模型中存在的全域統計資料為基礎。</span><span class="sxs-lookup"><span data-stu-id="9016d-127">If you choose a predictable attribute that does not meet the threshold for feature selection the attribute can still be used for prediction, but the predictions will be based solely on the global statistics that exist in the model.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="9016d-128">特徵選取只會影響用於模型的資料行，對採礦結構的儲存不會有任何影響。</span><span class="sxs-lookup"><span data-stu-id="9016d-128">Feature selection affects only the columns that are used in the model, and has no effect on storage of the mining structure.</span></span> <span data-ttu-id="9016d-129">排除在採礦模型外的資料行仍可用於結構，而採礦結構資料行中的資料則會存入快取。</span><span class="sxs-lookup"><span data-stu-id="9016d-129">The columns that you leave out of the mining model are still available in the structure, and data in the mining structure columns will be cached.</span></span>  
  
### <a name="definition-of-feature-selection-methods"></a><span data-ttu-id="9016d-130">特徵選取方法的定義</span><span class="sxs-lookup"><span data-stu-id="9016d-130">Definition of Feature Selection Methods</span></span>  
 <span data-ttu-id="9016d-131">依據您所使用的資料類型以及選擇用來分析的演算法而定，實作特徵選取的方法有許多種。</span><span class="sxs-lookup"><span data-stu-id="9016d-131">There are many ways to implement feature selection, depending on the type of data that you are working with and the algorithm that you choose for analysis.</span></span> <span data-ttu-id="9016d-132">SQL Server Analysis Services 提供數種常見且妥善建立的方法來為屬性評分。</span><span class="sxs-lookup"><span data-stu-id="9016d-132">SQL Server Analysis Services provides several popular and well-established methods for scoring attributes.</span></span> <span data-ttu-id="9016d-133">在任何演算法或資料集中所套用的方法取決於資料類型及資料行的使用方式。</span><span class="sxs-lookup"><span data-stu-id="9016d-133">The method that is applied in any algorithm or data set depends on the data types, and the column usage.</span></span>  
  
 <span data-ttu-id="9016d-134">「有趣性」\*\*(Interestingness) 分數是用來在包含非二進位連續數值資料的資料行中排名及排序屬性。</span><span class="sxs-lookup"><span data-stu-id="9016d-134">The *interestingness* score is used to rank and sort attributes in columns that contain nonbinary continuous numeric data.</span></span>  
  
 <span data-ttu-id="9016d-135">包含離散和離散化資料的資料行使用「Shannon 熵」**(Shannon's Entropy) 和兩個貝氏** (Bayesian) 分數。</span><span class="sxs-lookup"><span data-stu-id="9016d-135">*Shannon's entropy* and two *Bayesian* scores are available for columns that contain discrete and discretized data.</span></span> <span data-ttu-id="9016d-136">但是，如果模型包含任何連續資料行，則會使用有趣性分數評估所有輸入資料行，以確保一致性。</span><span class="sxs-lookup"><span data-stu-id="9016d-136">However, if the model contains any continuous columns, the interestingness score will be used to assess all input columns, to ensure consistency.</span></span>  
  
 <span data-ttu-id="9016d-137">下一節將描述每個特徵選取方法。</span><span class="sxs-lookup"><span data-stu-id="9016d-137">The following section describes each method of feature selection.</span></span>  
  
#### <a name="interestingness-score"></a><span data-ttu-id="9016d-138">有趣性分數</span><span class="sxs-lookup"><span data-stu-id="9016d-138">Interestingness score</span></span>  
 <span data-ttu-id="9016d-139">如果特徵能告訴您一些有用的資訊，就具備「有趣性」。</span><span class="sxs-lookup"><span data-stu-id="9016d-139">A feature is interesting if it tells you some useful piece of information.</span></span> <span data-ttu-id="9016d-140">由於這項功能的定義會因案例而有所不同，因此資料採礦產業已開發各種測量*有趣性*的方式。</span><span class="sxs-lookup"><span data-stu-id="9016d-140">Because the definition of what is useful varies depending on the scenario, the data mining industry has developed various ways to measure *interestingness*.</span></span> <span data-ttu-id="9016d-141">例如， *novelty*在極端值偵測中可能很有趣，但區分緊密相關專案或*群集辨識權數*的能力，可能會對分類更為感興趣。</span><span class="sxs-lookup"><span data-stu-id="9016d-141">For example, *novelty* might be interesting in outlier detection, but the ability to discriminate between closely related items, or *discriminating weight*, might be more interesting for classification.</span></span>  
  
 <span data-ttu-id="9016d-142">SQL Server Analysis Services 中使用的有趣性量值是以*熵為基礎*，這表示具有隨機散發的屬性具有較高的熵和較低的資訊增益;因此，這類屬性較不有趣。</span><span class="sxs-lookup"><span data-stu-id="9016d-142">The measure of interestingness that is used in SQL Server Analysis Services is *entropy-based*, meaning that attributes with random distributions have higher entropy and lower information gain; therefore, such attributes are less interesting.</span></span> <span data-ttu-id="9016d-143">任何特定屬性的熵都會與所有其他屬性的熵進行比較，如下所示：</span><span class="sxs-lookup"><span data-stu-id="9016d-143">The entropy for any particular attribute is compared to the entropy of all other attributes, as follows:</span></span>  
  
 <span data-ttu-id="9016d-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span><span class="sxs-lookup"><span data-stu-id="9016d-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span></span>  
  
 <span data-ttu-id="9016d-145">中間熵 (或稱 m) 代表整個功能集的熵。</span><span class="sxs-lookup"><span data-stu-id="9016d-145">Central entropy, or m, means the entropy of the entire feature set.</span></span> <span data-ttu-id="9016d-146">藉由從中間熵減去目標屬性的熵，您可以評估屬性所提供的資訊量。</span><span class="sxs-lookup"><span data-stu-id="9016d-146">By subtracting the entropy of the target attribute from the central entropy, you can assess how much information the attribute provides.</span></span>  
  
 <span data-ttu-id="9016d-147">每當資料行包含非二進位的連續數值資料時，依預設就會使用此分數。</span><span class="sxs-lookup"><span data-stu-id="9016d-147">This score is used by default whenever the column contains nonbinary continuous numeric data.</span></span>  
  
#### <a name="shannons-entropy"></a><span data-ttu-id="9016d-148">Shannon 熵</span><span class="sxs-lookup"><span data-stu-id="9016d-148">Shannon's Entropy</span></span>  
 <span data-ttu-id="9016d-149">Shannon 熵可針對特定結果而測量隨機變數的不確定性。</span><span class="sxs-lookup"><span data-stu-id="9016d-149">Shannon's entropy measures the uncertainty of a random variable for a particular outcome.</span></span> <span data-ttu-id="9016d-150">例如，擲銅板的熵可以表示為銅板呈現大頭的機率函數。</span><span class="sxs-lookup"><span data-stu-id="9016d-150">For example, the entropy of a coin toss can be represented as a function of the probability of it coming up heads.</span></span>  
  
 <span data-ttu-id="9016d-151">Analysis Services 使用下列公式來計算 Shannon 熵：</span><span class="sxs-lookup"><span data-stu-id="9016d-151">Analysis Services uses the following formula to calculate Shannon's entropy:</span></span>  
  
 <span data-ttu-id="9016d-152">H(X) = -∑ P(xi) log(P(xi))</span><span class="sxs-lookup"><span data-stu-id="9016d-152">H(X) = -∑ P(xi) log(P(xi))</span></span>  
  
 <span data-ttu-id="9016d-153">這個計分方法可用於分隔和離散化的屬性。</span><span class="sxs-lookup"><span data-stu-id="9016d-153">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-with-k2-prior"></a><span data-ttu-id="9016d-154">使用 K2 優先的貝氏</span><span class="sxs-lookup"><span data-stu-id="9016d-154">Bayesian with K2 Prior</span></span>  
 <span data-ttu-id="9016d-155">Analysis Services 提供兩種以貝氏網路為基礎的特徵選取分數。</span><span class="sxs-lookup"><span data-stu-id="9016d-155">Analysis Services provides two feature selection scores that are based on Bayesian networks.</span></span> <span data-ttu-id="9016d-156">貝氏網路是狀態及狀態間轉換的「導向」\*\* 或「非循環」\*\* 圖表，代表某些狀態一定會在目前的狀態之前、某些狀態會在之後，而圖表並不會重複或迴圈。</span><span class="sxs-lookup"><span data-stu-id="9016d-156">A Bayesian network is a *directed* or *acyclic* graph of states and transitions between states, meaning that some states are always prior to the current state, some states are posterior, and the graph does not repeat or loop.</span></span> <span data-ttu-id="9016d-157">依照定義，貝氏網路可以使用先前的知識。</span><span class="sxs-lookup"><span data-stu-id="9016d-157">By definition, Bayesian networks allow the use of prior knowledge.</span></span> <span data-ttu-id="9016d-158">不過，在計算稍後狀態的機率時要使用什麼先前狀態的問題，對於演算法的設計、效能和精確度都很重要。</span><span class="sxs-lookup"><span data-stu-id="9016d-158">However, the question of which prior states to use in calculating probabilities of later states is important for algorithm design, performance, and accuracy.</span></span>  
  
 <span data-ttu-id="9016d-159">貝氏網路學習的 K2 演算法是由 Cooper 和 Herskovits 所開發，常用於資料採礦中。</span><span class="sxs-lookup"><span data-stu-id="9016d-159">The K2 algorithm for learning from a Bayesian network was developed by Cooper and Herskovits and is often used in data mining.</span></span> <span data-ttu-id="9016d-160">這個演算法可以擴充，而且可以分析多個變數，但需要對當做輸入的變數進行排序。</span><span class="sxs-lookup"><span data-stu-id="9016d-160">It is scalable and can analyze multiple variables, but requires ordering on variables used as input.</span></span> <span data-ttu-id="9016d-161">如需詳細資訊，請參閱 Chickering、Geiger 和 Heckerman 所著的 [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) (學習貝氏網路)。</span><span class="sxs-lookup"><span data-stu-id="9016d-161">For more information, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) by Chickering, Geiger, and Heckerman.</span></span>  
  
 <span data-ttu-id="9016d-162">這個計分方法可用於分隔和離散化的屬性。</span><span class="sxs-lookup"><span data-stu-id="9016d-162">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-dirichlet-equivalent-with-uniform-prior"></a><span data-ttu-id="9016d-163">使用統一優先的貝氏狄氏等價</span><span class="sxs-lookup"><span data-stu-id="9016d-163">Bayesian Dirichlet Equivalent with Uniform Prior</span></span>  
 <span data-ttu-id="9016d-164">貝氏狄氏等價 (Bayesian Dirichlet Equivalent，BDE) 分數也會使用貝氏分析來評估具有資料集的網路。</span><span class="sxs-lookup"><span data-stu-id="9016d-164">The Bayesian Dirichlet Equivalent (BDE) score also uses Bayesian analysis to evaluate a network given a dataset.</span></span> <span data-ttu-id="9016d-165">BDE 計分方法是由 Heckerman 開發，以 Cooper 和 Herskovits 所開發的 BD 標準為基礎。</span><span class="sxs-lookup"><span data-stu-id="9016d-165">The BDE scoring method was developed by Heckerman and is based on the BD metric developed by Cooper and Herskovits.</span></span> <span data-ttu-id="9016d-166">Dirichlet 散發是多維度散發，描述網路中每個變數的條件式機率，而且具有許多對學習有用的屬性。</span><span class="sxs-lookup"><span data-stu-id="9016d-166">The Dirichlet distribution is a multinomial distribution that describes the conditional probability of each variable in the network, and has many properties that are useful for learning.</span></span>  
  
 <span data-ttu-id="9016d-167">使用統一優先的貝氏狄氏等價 (Bayesian Dirichlet Equivalent with Uniform Prior，BDEU) 方法假設狄氏散發的特殊案例，其中會使用數學常數來建立先前狀態的固定或統一散發。</span><span class="sxs-lookup"><span data-stu-id="9016d-167">The Bayesian Dirichlet Equivalent with Uniform Prior (BDEU) method assumes a special case of the Dirichlet distribution, in which a mathematical constant is used to create a fixed or uniform distribution of prior states.</span></span> <span data-ttu-id="9016d-168">BDE 分數也假設可能性相等，這代表不能預期資料會區別相等的結構。</span><span class="sxs-lookup"><span data-stu-id="9016d-168">The BDE score also assumes likelihood equivalence, which means that the data cannot be expected to discriminate equivalent structures.</span></span> <span data-ttu-id="9016d-169">換句話說，如果「If A Then B」的分數與「If B Then A」的分數相同，則不能根據資料區分結構，也不能推斷因果關係。</span><span class="sxs-lookup"><span data-stu-id="9016d-169">In other words, if the score for If A Then B is the same as the score for If B Then A, the structures cannot be distinguished based on the data, and causation cannot be inferred.</span></span>  
  
 <span data-ttu-id="9016d-170">如需貝氏網路以及實作這些計分方法的詳細資訊，請參閱 [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885)(學習貝氏網路)。</span><span class="sxs-lookup"><span data-stu-id="9016d-170">For more information about Bayesian networks and the implementation of these scoring methods, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span></span>  
  
### <a name="feature-selection-methods-used-by-analysis-services-algorithms"></a><span data-ttu-id="9016d-171">Analysis Services 演算法所使用的特徵選取方法</span><span class="sxs-lookup"><span data-stu-id="9016d-171">Feature Selection Methods used by Analysis Services Algorithms</span></span>  
 <span data-ttu-id="9016d-172">下表列出支援特徵選取的演算法、演算法所使用的特徵選取方法，以及設定來控制特徵選取行為的參數：</span><span class="sxs-lookup"><span data-stu-id="9016d-172">The following table lists the algorithms that support feature selection, the feature selection methods used by the algorithm, and the parameters that you set to control feature selection behavior:</span></span>  
  
|<span data-ttu-id="9016d-173">演算法</span><span class="sxs-lookup"><span data-stu-id="9016d-173">Algorithm</span></span>|<span data-ttu-id="9016d-174">分析的方法</span><span class="sxs-lookup"><span data-stu-id="9016d-174">Method of analysis</span></span>|<span data-ttu-id="9016d-175">註解</span><span class="sxs-lookup"><span data-stu-id="9016d-175">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="9016d-176">貝氏機率分類</span><span class="sxs-lookup"><span data-stu-id="9016d-176">Naive Bayes</span></span>|<span data-ttu-id="9016d-177">Shannon 熵</span><span class="sxs-lookup"><span data-stu-id="9016d-177">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="9016d-178">使用 K2 優先的貝氏</span><span class="sxs-lookup"><span data-stu-id="9016d-178">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="9016d-179">使用優先統一狄氏分配的貝氏 (預設值)</span><span class="sxs-lookup"><span data-stu-id="9016d-179">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="9016d-180">Microsoft 貝氏機率分類演算法只接受分隔或離散化的屬性，因此無法使用有趣性分數。</span><span class="sxs-lookup"><span data-stu-id="9016d-180">The Microsoft Naïve Bayes algorithm accepts only discrete or discretized attributes; therefore, it cannot use the interestingness score.</span></span><br /><br /> <span data-ttu-id="9016d-181">如需這個演算法的詳細資訊，請參閱 [Microsoft 貝氏機率分類演算法技術參考](microsoft-naive-bayes-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="9016d-181">For more information about this algorithm, see [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="9016d-182">決策樹</span><span class="sxs-lookup"><span data-stu-id="9016d-182">Decision trees</span></span>|<span data-ttu-id="9016d-183">有趣性分數</span><span class="sxs-lookup"><span data-stu-id="9016d-183">Interestingness score</span></span><br /><br /> <span data-ttu-id="9016d-184">Shannon 熵</span><span class="sxs-lookup"><span data-stu-id="9016d-184">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="9016d-185">使用 K2 優先的貝氏</span><span class="sxs-lookup"><span data-stu-id="9016d-185">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="9016d-186">使用優先統一狄氏分配的貝氏 (預設值)</span><span class="sxs-lookup"><span data-stu-id="9016d-186">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="9016d-187">如果任何資料行包含非二進位連續數值，則所有資料行都會使用有趣性分數以確保一致性。</span><span class="sxs-lookup"><span data-stu-id="9016d-187">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="9016d-188">否則，便會使用預設特徵選取方法，或是當您建立模型時所指定的方法。</span><span class="sxs-lookup"><span data-stu-id="9016d-188">Otherwise, the default feature selection method is used, or the method that you specified when you created the model.</span></span><br /><br /> <span data-ttu-id="9016d-189">如需這個演算法的詳細資訊，請參閱 [Microsoft 決策樹演算法技術參考](microsoft-decision-trees-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="9016d-189">For more information about this algorithm, see [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="9016d-190">類神經網路</span><span class="sxs-lookup"><span data-stu-id="9016d-190">Neural network</span></span>|<span data-ttu-id="9016d-191">有趣性分數</span><span class="sxs-lookup"><span data-stu-id="9016d-191">Interestingness score</span></span><br /><br /> <span data-ttu-id="9016d-192">Shannon 熵</span><span class="sxs-lookup"><span data-stu-id="9016d-192">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="9016d-193">使用 K2 優先的貝氏</span><span class="sxs-lookup"><span data-stu-id="9016d-193">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="9016d-194">使用優先統一狄氏分配的貝氏 (預設值)</span><span class="sxs-lookup"><span data-stu-id="9016d-194">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="9016d-195">Microsoft 類神經網路演算法可以使用貝氏和以熵為基礎的方法，只要資料包含連續資料行即可。</span><span class="sxs-lookup"><span data-stu-id="9016d-195">The Microsoft Neural Networks algorithm can use both Bayesian and entropy-based methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="9016d-196">如需這個演算法的詳細資訊，請參閱 [Microsoft 類神經網路演算法技術參考](microsoft-neural-network-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="9016d-196">For more information about this algorithm, see [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="9016d-197">羅吉斯迴歸</span><span class="sxs-lookup"><span data-stu-id="9016d-197">Logistic regression</span></span>|<span data-ttu-id="9016d-198">有趣性分數</span><span class="sxs-lookup"><span data-stu-id="9016d-198">Interestingness score</span></span><br /><br /> <span data-ttu-id="9016d-199">Shannon 熵</span><span class="sxs-lookup"><span data-stu-id="9016d-199">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="9016d-200">使用 K2 優先的貝氏</span><span class="sxs-lookup"><span data-stu-id="9016d-200">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="9016d-201">使用優先統一狄氏分配的貝氏 (預設值)</span><span class="sxs-lookup"><span data-stu-id="9016d-201">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="9016d-202">雖然 Microsoft 羅吉斯迴歸演算法是根據 Microsoft 類神經網路演算法，但是您無法自訂羅吉斯迴歸模型來控制特徵選取行為。因此，特徵選取一定會預設為最適合屬性的方法。</span><span class="sxs-lookup"><span data-stu-id="9016d-202">Although the Microsoft Logistic Regression algorithm is based on the Microsoft Neural Network algorithm, you cannot customize logistic regression models to control feature selection behavior; therefore, feature selection always default to the method that is most appropriate for the attribute.</span></span><br /><br /> <span data-ttu-id="9016d-203">如果所有屬性都是離散或離散化的，則預設值為 BDEU。</span><span class="sxs-lookup"><span data-stu-id="9016d-203">If all attributes are discrete or discretized, the default is BDEU.</span></span><br /><br /> <span data-ttu-id="9016d-204">如需這個演算法的詳細資訊，請參閱 [Microsoft 羅吉斯迴歸演算法技術參考](microsoft-logistic-regression-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="9016d-204">For more information about this algorithm, see [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="9016d-205">叢集</span><span class="sxs-lookup"><span data-stu-id="9016d-205">Clustering</span></span>|<span data-ttu-id="9016d-206">有趣性分數</span><span class="sxs-lookup"><span data-stu-id="9016d-206">Interestingness score</span></span>|<span data-ttu-id="9016d-207">Microsoft 群集演算法可以使用離散或離散化資料。</span><span class="sxs-lookup"><span data-stu-id="9016d-207">The Microsoft Clustering algorithm can use discrete or discretized data.</span></span> <span data-ttu-id="9016d-208">但是，由於每一個屬性的分數都會計算為距離，並表示為連續的數字，因此必須使用有趣性分數。</span><span class="sxs-lookup"><span data-stu-id="9016d-208">However, because the score of each attribute is calculated as a distance and is represented as a continuous number, the interestingness score must be used.</span></span><br /><br /> <span data-ttu-id="9016d-209">如需這個演算法的詳細資訊，請參閱 [Microsoft 群集演算法技術參考](microsoft-clustering-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="9016d-209">For more information about this algorithm, see [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="9016d-210">線性迴歸</span><span class="sxs-lookup"><span data-stu-id="9016d-210">Linear regression</span></span>|<span data-ttu-id="9016d-211">有趣性分數</span><span class="sxs-lookup"><span data-stu-id="9016d-211">Interestingness score</span></span>|<span data-ttu-id="9016d-212">Microsoft 線性迴歸演算法只能使用有趣性分數，因為它只支援連續的資料行。</span><span class="sxs-lookup"><span data-stu-id="9016d-212">The Microsoft Linear Regression algorithm can only use the interestingness score, because it only supports continuous columns.</span></span><br /><br /> <span data-ttu-id="9016d-213">如需這個演算法的詳細資訊，請參閱 [Microsoft 線性迴歸演算法技術參考](microsoft-linear-regression-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="9016d-213">For more information about this algorithm, see [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="9016d-214">關聯規則</span><span class="sxs-lookup"><span data-stu-id="9016d-214">Association rules</span></span><br /><br /> <span data-ttu-id="9016d-215">時序群集</span><span class="sxs-lookup"><span data-stu-id="9016d-215">Sequence clustering</span></span>|<span data-ttu-id="9016d-216">未使用</span><span class="sxs-lookup"><span data-stu-id="9016d-216">Not used</span></span>|<span data-ttu-id="9016d-217">不會使用這些演算法叫用特徵選取。</span><span class="sxs-lookup"><span data-stu-id="9016d-217">Feature selection is not invoked with these algorithms.</span></span><br /><br /> <span data-ttu-id="9016d-218">不過，您可以藉由設定 MINIMUM_SUPPORT 和 MINIMUM_PROBABILIITY 參數的值，控制演算法的行為，並在必要時減少輸入資料的大小。</span><span class="sxs-lookup"><span data-stu-id="9016d-218">However, you can control the behavior of the algorithm and reduce the size of input data if necessary by setting the value of the parameters MINIMUM_SUPPORT and MINIMUM_PROBABILIITY.</span></span><br /><br /> <span data-ttu-id="9016d-219">如需詳細資訊，請參閱 [Microsoft 關聯分析演算法技術參考](microsoft-association-algorithm-technical-reference.md) 和 [Microsoft 時序群集演算法技術參考](microsoft-sequence-clustering-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="9016d-219">For more information, see [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) and [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="9016d-220">時間序列</span><span class="sxs-lookup"><span data-stu-id="9016d-220">Time series</span></span>|<span data-ttu-id="9016d-221">未使用</span><span class="sxs-lookup"><span data-stu-id="9016d-221">Not used</span></span>|<span data-ttu-id="9016d-222">特徵選取不適用於時間序列模型。</span><span class="sxs-lookup"><span data-stu-id="9016d-222">Feature selection does not apply to time series models.</span></span><br /><br /> <span data-ttu-id="9016d-223">如需這個演算法的詳細資訊，請參閱 [Microsoft 時間序列演算法技術參考](microsoft-time-series-algorithm-technical-reference.md)。</span><span class="sxs-lookup"><span data-stu-id="9016d-223">For more information about this algorithm, see [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span></span>|  
  
## <a name="feature-selection-parameters"></a><span data-ttu-id="9016d-224">特徵選取參數</span><span class="sxs-lookup"><span data-stu-id="9016d-224">Feature Selection Parameters</span></span>  
 <span data-ttu-id="9016d-225">在支援特徵選取的演算法中，您可以使用下列參數來控制何時開啟特徵選取。</span><span class="sxs-lookup"><span data-stu-id="9016d-225">In algorithms that support feature selection, you can control when feature selection is turned on by using the following parameters.</span></span> <span data-ttu-id="9016d-226">每個演算法都具有所允許之輸入數目的預設值，但是您可以覆寫這個預設值並指定屬性的數目。</span><span class="sxs-lookup"><span data-stu-id="9016d-226">Each algorithm has a default value for the number of inputs that are allowed, but you can override this default and specify the number of attributes.</span></span> <span data-ttu-id="9016d-227">本節列出為管理特徵選取所提供的參數。</span><span class="sxs-lookup"><span data-stu-id="9016d-227">This section lists the parameters that are provided for managing feature selection.</span></span>  
  
#### <a name="maximum_input_attributes"></a><span data-ttu-id="9016d-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="9016d-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="9016d-229">如果模型包含的資料行比在 *MAXIMUM_INPUT_ATTRIBUTES* 參數中所指定的數目多，則演算法會忽略認為不重要的任何資料行。</span><span class="sxs-lookup"><span data-stu-id="9016d-229">If a model contains more columns than the number that is specified in the *MAXIMUM_INPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_output_attributes"></a><span data-ttu-id="9016d-230">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="9016d-230">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="9016d-231">同樣地，如果模型包含的可預測資料行比在 *MAXIMUM_OUTPUT_ATTRIBUTES* 參數中所指定的數目多，則演算法會忽略認為不重要的任何資料行。</span><span class="sxs-lookup"><span data-stu-id="9016d-231">Similarly, if a model contains more predictable columns than the number that is specified in the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_states"></a><span data-ttu-id="9016d-232">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="9016d-232">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="9016d-233">如果模型包含的案例比在 *MAXIMUM_STATES* 參數中所指定的數目多，則會將最不常用的狀態群組在一起，並視為遺漏。</span><span class="sxs-lookup"><span data-stu-id="9016d-233">If a model contains more cases than are specified in the *MAXIMUM_STATES* parameter, the least popular states are grouped together and treated as missing.</span></span> <span data-ttu-id="9016d-234">如果其中有任何參數設定為 0，特徵選取就會關閉，且會影響處理時間和效能。</span><span class="sxs-lookup"><span data-stu-id="9016d-234">If any one of these parameters is set to 0, feature selection is turned off, affecting processing time and performance.</span></span>  
  
 <span data-ttu-id="9016d-235">除了這些特徵選取方法之外，您也可以透過在模型上設定「模型旗標」**，或透過在結構上設定「散發旗標」**，來改進演算法識別或提升有意義屬性的能力。</span><span class="sxs-lookup"><span data-stu-id="9016d-235">In addition to these methods for feature selection, you can improve the ability of the algorithm to identify or promote meaningful attributes by setting *modeling flags* on the model or by setting *distribution flags* on the structure.</span></span> <span data-ttu-id="9016d-236">如需這些概念的詳細資訊，請參閱[模型旗標 &#40;資料採礦&#41;](modeling-flags-data-mining.md) 和[資料行散發 &#40;資料採礦&#41;](column-distributions-data-mining.md)。</span><span class="sxs-lookup"><span data-stu-id="9016d-236">For more information about these concepts, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md) and [Column Distributions &#40;Data Mining&#41;](column-distributions-data-mining.md).</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="9016d-237">另請參閱</span><span class="sxs-lookup"><span data-stu-id="9016d-237">See Also</span></span>  
 [<span data-ttu-id="9016d-238">自訂採礦模型和結構</span><span class="sxs-lookup"><span data-stu-id="9016d-238">Customize Mining Models and Structure</span></span>](customize-mining-models-and-structure.md)  
  
  
